{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymongo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymongo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MongoClient, database, collection\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpymongo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConnectionFailure, OperationFailure\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m contextmanager\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pymongo'"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient, database, collection\n",
    "from pymongo.errors import ConnectionFailure, OperationFailure\n",
    "from contextlib import contextmanager\n",
    "import pandas as pd\n",
    "\"\"\" Context manager for mongoDB connection. \"\"\"\n",
    "@contextmanager\n",
    "def mongoDB_client(username: str, password: str, \n",
    "                    host: str = 'mongo', port: str = 27017):\n",
    "    #set path\n",
    "    path = f\"mongodb://{username}:{password}@{host}:{port}\"\n",
    "    client = None\n",
    "\n",
    "    #init\n",
    "    try:\n",
    "        print(\"Starting connect mongoDB...\")\n",
    "        client = MongoClient(path)\n",
    "        \n",
    "        print(\"Client connected successfully!\")\n",
    "        yield client\n",
    "\n",
    "    #handle error\n",
    "    except ConnectionFailure:\n",
    "        print(\"Connection to mongoDB failed!\")\n",
    "\n",
    "    except OperationFailure:\n",
    "        print(\"Operation failed!\")\n",
    "\n",
    "    #close client\n",
    "    finally:\n",
    "        client.close()\n",
    "        print(\"The connection to MongoDB has stopped!\")\n",
    "\n",
    "\"\"\" Class mongoDB for operations. \"\"\"\n",
    "class mongoDB_operations:\n",
    "    \"\"\" Init \"\"\"\n",
    "    def __init__(self, client: MongoClient):\n",
    "        #check params\n",
    "        if not isinstance(client, MongoClient):\n",
    "            raise TypeError('client must be MongoClient!')\n",
    "        \n",
    "        #set value for class attrs\n",
    "        self.client = client\n",
    "\n",
    "    \"\"\" Check whether the database exists. \"\"\"\n",
    "    def check_database_exists(self, database_name: str) -> bool:\n",
    "        #list database name\n",
    "        return database_name in self.client.list_database_names()\n",
    "\n",
    "    \"\"\" Check whether collection exists. \"\"\"\n",
    "    def check_collection_exists(self, database_obj: database.Database, collection: str) -> bool:\n",
    "        #check params\n",
    "        if not isinstance(database_obj, database.Database):\n",
    "            raise TypeError(\"database_obj must be a database.Database!\")\n",
    "        \n",
    "        #list collection name\n",
    "        return collection in self.client[database_obj.name].list_collection_names()\n",
    "\n",
    "    \"\"\" Create new database. \"\"\"\n",
    "    def create_database_if_not_exists(self, database_name: str) -> database.Database:\n",
    "        #check whether database exists\n",
    "        if self.check_database_exists(database_name):\n",
    "            print(f\"Don't create the database '{database_name}' because it already exists.\")\n",
    "        else:\n",
    "            print(f\"Successfully created database '{database_name}'.\")\n",
    "\n",
    "        #return database\n",
    "        return self.client[database_name]\n",
    "    \n",
    "    \"\"\" Create new collection. \"\"\"\n",
    "    def create_collection_if_not_exists(self, database_obj: database.Database, collection: str) -> collection.Collection:\n",
    "        #check params\n",
    "        if not isinstance(database_obj, database.Database):\n",
    "            raise TypeError(\"database_obj must be a database.Database!\")\n",
    "        \n",
    "        #check whether collection exists\n",
    "        if self.check_collection_exists(database_obj, collection):\n",
    "            print(f\"Don't create the collection '{collection}' because it already exists.\")\n",
    "        else:\n",
    "            print(f\"Successfully created collection '{collection}'.\")\n",
    "\n",
    "        #return collection\n",
    "        return self.client[database_obj.name][collection]\n",
    "    \n",
    "    \"\"\" Insert data. \"\"\"\n",
    "    def insert_data(self, database_name: str, collection_name: str, data = pd.DataFrame):\n",
    "        #check params\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            raise TypeError(\"data must be a DataFrame!\")\n",
    "        \n",
    "        database_obj = self.create_database_if_not_exists(database_name)\n",
    "        collection_obj = self.create_collection_if_not_exists(database_obj, collection_name)\n",
    "        #insert data\n",
    "        data = data.to_dict(orient = 'records')\n",
    "        collection_obj.insert_many(data)\n",
    "\n",
    "        print(f\"Successfully inserted data into collection '{collection_obj.name}'.\")\n",
    "    \n",
    "    \"\"\" Read data. \"\"\"\n",
    "    def read_data(self, database_name: str, collection_name:str, query: dict = None) -> pd.DataFrame:\n",
    "        #check params\n",
    "        if query is not None and not isinstance(query, dict):\n",
    "            raise TypeError(\"query must be a dict!\")\n",
    "        \n",
    "        #check database and collection exist\n",
    "        if not self.check_database_exists(database_name):\n",
    "            raise Exception(f\"Database '{database_name}' does not exist!\")\n",
    "        if not self.check_collection_exists(database_obj = self.client[database_name], collection = collection_name):\n",
    "            raise Exception(f\"Collection '{collection_name}' does not exist!\")\n",
    "        \n",
    "\n",
    "        data = self.client[database_name][collection_name].find(query)\n",
    "        data = pd.DataFrame(list(data))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting connect mongoDB...\n",
      "Client connected successfully!\n",
      "Don't create the database 'artist_database' because it already exists.\n",
      "Don't create the collection 'artist_collection' because it already exists.\n",
      "Successfully inserted data into collection 'artist_collection'.\n",
      "The connection to MongoDB has stopped!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\"\"\" Convert data to dictionaries. \"\"\"\n",
    "def get_dict_data(csv_path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    df = df.to_dict(orient = 'records')\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_mongodb_artist(artist_path: str = '/opt/data/Artist.csv'):\n",
    "    #use mongoDB client\n",
    "    with mongoDB_client(username = 'huynhthuan', password = 'password') as client:\n",
    "        client = mongoDB_operations(client)\n",
    "        #create artist database\n",
    "        client_artist_database = client.create_database_if_not_exists(database_name= 'artist_database')\n",
    "\n",
    "        #create artist collection\n",
    "        client_artist_collection = client.create_collection_if_not_exists(database_obj = client_artist_database, \n",
    "                                                                          collection = 'artist_collection')\n",
    "\n",
    "        #get data\n",
    "        data = get_dict_data(artist_path)    \n",
    "\n",
    "        #insert artist data\n",
    "        client_artist_insert = client.insert_data(collection_obj = client_artist_collection, data = data)\n",
    "\n",
    "load_mongodb_artist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, DateType, FloatType\n",
    "\n",
    "\"\"\" Function for getting schemas. \"\"\"\n",
    "def get_schema(table_name: str) -> StructType:\n",
    "    \"\"\" Artist schema. \"\"\"\n",
    "    artist_schema = [StructField('Artist_ID',      StringType(), True),\n",
    "                     StructField('Artist_Name',    StringType(), True),\n",
    "                     StructField('Genres',         ArrayType(StringType(), True), True),\n",
    "                     StructField('Followers',      IntegerType(), True),\n",
    "                     StructField('Popularity',     IntegerType(), True),\n",
    "                     StructField('Artist_Image',   StringType(), True),\n",
    "                     StructField('Artist_Type',    StringType(), True),\n",
    "                     StructField('External_Url',   StringType(), True),\n",
    "                     StructField('Href',           StringType(), True),\n",
    "                     StructField('Artist_Uri',     StringType(), True),\n",
    "                     StructField('Execution_date',  DateType(), True)]\n",
    "    #applying struct type\n",
    "    artist_schema = StructType(artist_schema)\n",
    "    \n",
    "    \"\"\" Album schema. \"\"\"\n",
    "    album_schema = [StructField('Artist',               StringType(), True),\n",
    "                    StructField('Artist_ID',            StringType(), True),\n",
    "                    StructField('Album_ID',             StringType(), True),\n",
    "                    StructField('Name',                 StringType(), True),\n",
    "                    StructField('Type',                 StringType(), True),\n",
    "                    StructField('Genres',               ArrayType(StringType(), True), True),\n",
    "                    StructField('Label',                StringType(), True),\n",
    "                    StructField('Popularity',           StringType(), True),\n",
    "                    StructField('Available_Markets',    StringType(), True),\n",
    "                    StructField('Release_Date',         DateType(), True),\n",
    "                    StructField('ReleaseDatePrecision', StringType(), True),\n",
    "                    StructField('TotalTracks',          IntegerType(), True),\n",
    "                    StructField('Copyrights',           StringType(), True),\n",
    "                    StructField('Restrictions',         StringType(), True),\n",
    "                    StructField('External_URL',         StringType(), True),\n",
    "                    StructField('Href',                 StringType(), True),\n",
    "                    StructField('Image',                StringType(), True),\n",
    "                    StructField('Uri',                  StringType(), True),\n",
    "                    StructField('Execution_date',        DateType(), True)]\n",
    "    #Applying struct type\n",
    "    album_schema = StructType(album_schema)\n",
    "\n",
    "    \"\"\" Track schema. \"\"\"\n",
    "    track_schema = [StructField(\"Artists\",          StringType(), True),\n",
    "                    StructField(\"Album_ID\",         StringType(), True),\n",
    "                    StructField(\"Album_Name\",       StringType(), True),\n",
    "                    StructField(\"Track_ID\",         StringType(), True),\n",
    "                    StructField(\"Name\",             StringType(), True),\n",
    "                    StructField(\"Track_Number\",     IntegerType(), True),\n",
    "                    StructField(\"Type\",             StringType(), True),\n",
    "                    StructField(\"AvailableMarkets\", StringType(), True),\n",
    "                    StructField(\"Disc_Number\",      StringType(), True),\n",
    "                    StructField(\"Duration_ms\",      IntegerType(), True),\n",
    "                    StructField(\"Explicit\",         StringType(), True),\n",
    "                    StructField(\"External_urls\",    StringType(), True),\n",
    "                    StructField(\"Href\",             StringType(), True),\n",
    "                    StructField(\"Restrictions\",     StringType(), True),\n",
    "                    StructField(\"Preview_url\",      StringType(), True),\n",
    "                    StructField(\"Uri\",              StringType(), True),\n",
    "                    StructField(\"Is_Local\",         StringType(), True),\n",
    "                    StructField('Execution_date',   DateType(), True)]\n",
    "    #Applying struct type\n",
    "    track_schema = StructType(track_schema)\n",
    "    \n",
    "    \"\"\" TrackFeature schema. \"\"\"\n",
    "    trackfeature_schema = [StructField(\"Track_ID\",         StringType(), True),\n",
    "                           StructField(\"Danceability\",     FloatType(), True),\n",
    "                           StructField(\"Energy\",           FloatType(), True),\n",
    "                           StructField(\"Key\",              IntegerType(), True),\n",
    "                           StructField(\"Loudness\",         FloatType(), True),\n",
    "                           StructField(\"Mode\",             IntegerType(), True),\n",
    "                           StructField(\"Speechiness\",      FloatType(), True),\n",
    "                           StructField(\"Acousticness\",     FloatType(), True),\n",
    "                           StructField(\"Instrumentalness\", FloatType(), True),\n",
    "                           StructField(\"Liveness\",         FloatType(), True),\n",
    "                           StructField(\"Valence\",          FloatType(), True),\n",
    "                           StructField(\"Tempo\",            FloatType(), True),\n",
    "                           StructField(\"Time_signature\",   IntegerType(), True),\n",
    "                           StructField(\"Track_href\",       StringType(), True),\n",
    "                           StructField(\"Type_Feature\",     StringType(), True),\n",
    "                           StructField(\"Analysis_Url\",     StringType(), True),\n",
    "                           StructField(\"Execution_date\",   StringType(), True)]\n",
    "    #Applying struct type\n",
    "    trackfeature_schema = StructType(trackfeature_schema)\n",
    "\n",
    "    #mapping\n",
    "    mapping = {\n",
    "        'artist': artist_schema,\n",
    "        'album': album_schema,\n",
    "        'track': track_schema,\n",
    "        'trackfeature': trackfeature_schema\n",
    "    }\n",
    "    \n",
    "    #return schema\n",
    "    return mapping[table_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkConf\n",
    "from contextlib import contextmanager\n",
    "import pyspark.sql\n",
    "\n",
    "\"\"\" Context manager for creating Spark Session. \"\"\"\n",
    "@contextmanager\n",
    "def get_sparkSession(appName: str, master: str = 'local'):\n",
    "    #declare sparkconf\n",
    "    conf = SparkConf()\n",
    "\n",
    "    #set config\n",
    "    conf = conf.setAppName(appName) \\\n",
    "               .setMaster(master) \\\n",
    "               .set(\"spark.executor.memory\", \"4g\") \\\n",
    "               .set(\"spark.executor.cores\", \"2\") \\\n",
    "               .set(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "               .set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "               .set(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.4.0\")\n",
    "               #.set(\"spark.jars.packages\", \"net.snowflake:spark-snowflake_2.12:2.12.0-spark_3.4\")\n",
    "    \n",
    "    #               .set(\"spark.executor.instances\", \"2\") \\\n",
    "    #create Spark Session\n",
    "    spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "\n",
    "    print(f\"Successfully created Spark Session with app name: {appName} and master: {master}!\")\n",
    "\n",
    "    #yield spark\n",
    "    try:\n",
    "        yield spark\n",
    "\n",
    "    finally:\n",
    "        #must stop Spark Session\n",
    "        spark.stop()\n",
    "        print(\"Successfully stopped Spark Session!\")\n",
    "\n",
    "\n",
    "\"\"\" Read data from mongoDB. \"\"\"\n",
    "def read_mongoDB(spark: SparkSession, database_name: str, collection_name: str, query: dict = None,\n",
    "                 schema: StructType = None, username: str = 'huynhthuan', password: str = 'password', \n",
    "                 host: str = 'mongo', port: str = 27017) -> pyspark.sql.DataFrame:\n",
    "    \n",
    "    #check params\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a SparkSession!\")\n",
    "    \n",
    "    if query is not None and not isinstance(query, dict):\n",
    "        raise TypeError(\"query must be a dict!\")\n",
    "    \n",
    "    if schema is not None and not isinstance(schema, StructType):\n",
    "        raise TypeError(\"schema must be a StructType!\")\n",
    "    \n",
    "    #uri mongoDB \n",
    "    uri = f\"mongodb://{username}:{password}@{host}:{port}/{database_name}.{collection_name}?authSource=admin\"\n",
    "\n",
    "    print(f\"Starting to read data from database '{database_name}' and collection '{collection_name}'...\")\n",
    "  \n",
    "    #read data\n",
    "    try:\n",
    "        data = spark.read.format('mongodb') \\\n",
    "                         .option(\"spark.mongodb.read.connection.uri\", uri) \\\n",
    "                         .option('header', 'true')\n",
    "        \n",
    "        data = data.schema(schema).load() if schema is not None else data.load()\n",
    "\n",
    "        return data \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading data from mongoDB: {e}\")\n",
    "\n",
    "\n",
    "\"\"\" Read data from HDFS. \"\"\"\n",
    "def read_HDFS(spark: SparkSession, HDFS_dir: str, file_type: str) -> pyspark.sql.DataFrame:\n",
    "    #check params\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a SparkSession!\")\n",
    "    \n",
    "    #set HDFS path\n",
    "    HDFS_path = f\"hdfs://namenode:9000/datalake/{HDFS_dir}\"\n",
    "\n",
    "    print(f\"Starting to read data from {HDFS_path}...\")\n",
    "\n",
    "    #read data\n",
    "    try:\n",
    "        data = spark.read.format(file_type).option('header', 'true').load(HDFS_path)\n",
    "        #return data\n",
    "        return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading data from HDFS: {e}\")\n",
    "\n",
    "\n",
    "\"\"\" Write data into HDFS. \"\"\"\n",
    "def write_HDFS(spark: SparkSession, data: pyspark.sql.DataFrame, direct: str, file_type: str, partition: str):\n",
    "    #check params\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a SparkSession!\")\n",
    "    \n",
    "    if not isinstance(data, pyspark.sql.DataFrame):\n",
    "        raise TypeError(\"data must be a DataFrame!\")\n",
    "\n",
    "    #set HDFS path  \n",
    "    HDFS_path = f\"hdfs://namenode:9000/datalake/{direct}\"\n",
    "    table_name = direct.split('/')[-1]\n",
    "\n",
    "    print(f\"Starting to upload '{table_name}' into {HDFS_path}...\")\n",
    "    \n",
    "    #write data\n",
    "    try:\n",
    "        if partition is not None:\n",
    "            data.write.format(file_type) \\\n",
    "                      .option('header', 'true') \\\n",
    "                      .mode('append') \\\n",
    "                      .partitionBy('Execution_date') \\\n",
    "                      .save(HDFS_path)\n",
    "        else:\n",
    "            data.write.format(file_type) \\\n",
    "                      .option('header', 'true') \\\n",
    "                      .mode('append') \\\n",
    "                      .save(HDFS_path)\n",
    "        \n",
    "        print(f\"Successfully uploaded '{table_name}' into HDFS.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while upload data into HDFS: {e}\")\n",
    "\n",
    "\"\"\" Write data into SnowFlake Data Warehouse. \"\"\"\n",
    "def write_SnowFlake(spark: SparkSession, data: pyspark.sql.DataFrame, table_name: str):\n",
    "    #check params\n",
    "    if not isinstance(spark, SparkSession):\n",
    "        raise TypeError(\"spark must be a SparkSession!\")\n",
    "    \n",
    "    if not isinstance(data, pyspark.sql.DataFrame):\n",
    "        raise TypeError(\"data must be a DataFrame!\")\n",
    "    \n",
    "    snowflake_connection_options = {\n",
    "        \"sfURL\": \"https://sl70006.southeast-asia.azure.snowflakecomputing.com\",\n",
    "        \"sfUser\": \"HUYNHTHUAN\", \n",
    "        \"sfPassword\": \"Thuan123456\",\n",
    "        \"sfWarehouse\": \"COMPUTE_WH\",\n",
    "        \"sfDatabase\": \"SPOTIFY_MUSIC_DB\" \n",
    "    }\n",
    "\n",
    "    print(f\"Starting to upload {table_name.split('.')[-1]} into SnowFlake...\")\n",
    "    try:\n",
    "        data.write.format(\"snowflake\") \\\n",
    "                .options(**snowflake_connection_options) \\\n",
    "                .option(\"dbtable\", table_name) \\\n",
    "                .mode('append') \\\n",
    "                .save()\n",
    "        print(f\"Successfully uploaded '{table_name}' into SnowFlake.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while upload data into HDFS: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e0967f68-534b-4c7b-8942-d315d202c7ec;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.4.0 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;5.1.4 in central\n",
      "\t[5.1.4] org.mongodb#mongodb-driver-sync;[5.1.1,5.1.99)\n",
      "\tfound org.mongodb#bson;5.1.4 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;5.1.4 in central\n",
      "\tfound org.mongodb#bson-record-codec;5.1.4 in central\n",
      ":: resolution report :: resolve 7380ms :: artifacts dl 43ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;5.1.4 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;5.1.4 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.4.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   1   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e0967f68-534b-4c7b-8942-d315d202c7ec\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/21ms)\n",
      "24/11/21 06:18:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Spark Session with app name: init_load and master: local!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully stopped Spark Session!\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Load all csv files into mongoDB.\"\"\"\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import lit\n",
    "if __name__ == \"__main__\":\n",
    "    with get_sparkSession(appName = \"init_load\") as spark:\n",
    "        execution_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        #uri\n",
    "        uri_artist_name = \"mongodb://huynhthuan:password@mongo:27017/music_database.artist_name_collection?authSource=admin\"\n",
    "        uri_artist = \"mongodb://huynhthuan:password@mongo:27017/music_database.artist_collection?authSource=admin\"\n",
    "        uri_album = \"mongodb://huynhthuan:password@mongo:27017/music_database.album_collection?authSource=admin\"\n",
    "        uri_track = \"mongodb://huynhthuan:password@mongo:27017/music_database.track_collection?authSource=admin\"\n",
    "        uri_trackfeature = \"mongodb://huynhthuan:password@mongo:27017/music_database.trackfeature_collection?authSource=admin\"\n",
    "\n",
    "        # read\n",
    "        df_ArtistName = spark.read.option('header', 'true').csv(\"/opt/data/ArtistName.csv\")\n",
    "        df_ArtistName = df_ArtistName.withColumn('Execution_date', lit(execution_date))\n",
    "        \n",
    "        df_Artist = spark.read.option('header', 'true').csv(\"/opt/data/Artist.csv\")\n",
    "        df_Artist = df_Artist.withColumn('Execution_date', lit(execution_date))\n",
    "\n",
    "        df_Album = spark.read.option('header', 'true').csv(\"/opt/data/Album.csv\")\n",
    "        df_Album = df_Album.withColumn('Execution_date', lit(execution_date))\n",
    "\n",
    "        df_Track = spark.read.option('header', 'true').csv(\"/opt/data/Track.csv\")\n",
    "        df_Track = df_Track.withColumn('Execution_date', lit(execution_date))\n",
    "        \n",
    "        df_TrackFeature = spark.read.option('header', 'true').csv(\"/opt/data/TrackFeature.csv\")\n",
    "        df_TrackFeature = df_TrackFeature.withColumn('Execution_date', lit(execution_date))\n",
    "\n",
    "        #write\n",
    "        df_ArtistName.write.format('mongoDB') \\\n",
    "                           .option(\"spark.mongodb.write.connection.uri\", uri_artist_name) \\\n",
    "                           .mode(\"overwrite\") \\\n",
    "                           .save()\n",
    "        \n",
    "        df_Artist.write.format('mongoDB') \\\n",
    "                       .option(\"spark.mongodb.write.connection.uri\", uri_artist) \\\n",
    "                       .mode(\"overwrite\") \\\n",
    "                       .save()\n",
    "        \n",
    "        df_Album.write.format('mongoDB') \\\n",
    "                       .option(\"spark.mongodb.write.connection.uri\", uri_album) \\\n",
    "                       .mode(\"overwrite\") \\\n",
    "                       .save()\n",
    "        \n",
    "        df_Track.write.format('mongoDB') \\\n",
    "                      .option(\"spark.mongodb.write.connection.uri\", uri_track) \\\n",
    "                      .mode(\"overwrite\") \\\n",
    "                      .save()\n",
    "        \n",
    "        df_TrackFeature.write.format('mongoDB') \\\n",
    "                             .option(\"spark.mongodb.write.connection.uri\", uri_trackfeature) \\\n",
    "                             .mode(\"overwrite\") \\\n",
    "                             .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------- Bronze task starts! -------------------------------\n",
      "Successfully created Spark Session with app name: Bronze_task and master: local!\n",
      "Starting to read data from database 'music_database' and collection 'artist_collection'...\n",
      "+--------------------+--------------------+-----------+-----------+--------------------+--------------+--------------------+---------+--------------------+--------------------+----------+--------------------+\n",
      "|           Artist_ID|        Artist_Image|Artist_Name|Artist_Type|          Artist_Uri|Execution_date|        External_Url|Followers|              Genres|                Href|Popularity|                 _id|\n",
      "+--------------------+--------------------+-----------+-----------+--------------------+--------------+--------------------+---------+--------------------+--------------------+----------+--------------------+\n",
      "|3tVQdUvClmAT7URs9...|https://i.scdn.co...|     Wizkid|     artist|spotify:artist:3t...|    2024-11-23|{\"spotify\": \"http...|  6431584|afrobeats, afropo...|https://api.spoti...|        81|674203458cad448f3...|\n",
      "+--------------------+--------------------+-----------+-----------+--------------------+--------------+--------------------+---------+--------------------+--------------------+----------+--------------------+\n",
      "\n",
      "Starting bronze preprocessing for artist data...\n",
      "DataFrame[Artist_ID: string, Artist_Name: string, Genres: array<string>, Followers: int, Popularity: int, Artist_Image: string, Artist_Type: string, External_Url: string, Href: string, Artist_Uri: string, Execution_date: date]\n",
      "Finished bronze preprocessing for artist data.\n",
      "Starting to upload 'bronze_artist' into hdfs://namenode:9000/datalake/bronze_data/bronze_artist...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'bronze_artist' into HDFS.\n",
      "Starting to read data from database 'music_database' and collection 'album_collection'...\n",
      "+--------------------+------+--------------------+--------------------+--------------------+--------------+--------------------+------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+------------+------------+-----------+-----+--------------------+--------------------+\n",
      "|            Album_ID|Artist|           Artist_ID|   Available_Markets|          Copyrights|Execution_date|        External_URL|Genres|                Href|               Image|               Label|                Name|Popularity|ReleaseDatePrecision|Release_Date|Restrictions|TotalTracks| Type|                 Uri|                 _id|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+--------------+--------------------+------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+------------+------------+-----------+-----+--------------------+--------------------+\n",
      "|3dLXfyaG1kYeSQknL...|Wizkid|3tVQdUvClmAT7URs9...|AR,AU,AT,BE,BO,BR...|(P) 2024 RCA Reco...|    2024-11-23|https://open.spot...|      |https://api.spoti...|https://i.scdn.co...| Starboy/RCA Records|              Morayo|        60|                 day|  2024-11-22|        null|         16|album|spotify:album:3dL...|6742034f1f8786457...|\n",
      "|73rKiFhHZatrwJL0B...|Wizkid|3tVQdUvClmAT7URs9...|AR,AU,AT,BE,BO,BR...|(P) 2022 Starboy ...|    2024-11-23|https://open.spot...|      |https://api.spoti...|https://i.scdn.co...| Starboy/RCA Records| More Love, Less Ego|        67|                 day|  2022-11-11|        null|         13|album|spotify:album:73r...|6742034f1f8786457...|\n",
      "|6bCs4XCCkm9cTwlsw...|Wizkid|3tVQdUvClmAT7URs9...|AR,AU,AT,BE,BO,BR...|(P) 2021 Starboy ...|    2024-11-23|https://open.spot...|      |https://api.spoti...|https://i.scdn.co...| Starboy/RCA Records|Made In Lagos: De...|        64|                 day|  2021-08-27|        null|         18|album|spotify:album:6bC...|6742034f1f8786457...|\n",
      "|6HpMdN52TfJAwVbmk...|Wizkid|3tVQdUvClmAT7URs9...|AR,AU,AT,BE,BO,BR...|(P) 2020 Starboy ...|    2024-11-23|https://open.spot...|      |https://api.spoti...|https://i.scdn.co...| Starboy/RCA Records|       Made In Lagos|        72|                 day|  2020-10-29|        null|         14|album|spotify:album:6Hp...|6742034f1f8786457...|\n",
      "|2yUhcn7kF408KjNVu...|Wizkid|3tVQdUvClmAT7URs9...|AR,AU,AT,BE,BO,BR...|(P) 2017 Starboy ...|    2024-11-23|https://open.spot...|      |https://api.spoti...|https://i.scdn.co...| Starboy/RCA Records|Sounds From The O...|        59|                 day|  2017-07-14|        null|         12|album|spotify:album:2yU...|6742034f1f8786457...|\n",
      "|3K4CaKaEcLuJkJZ3l...|Wizkid|3tVQdUvClmAT7URs9...|AR,AU,AT,BE,BO,BR...|2014 Empire Mates...|    2024-11-23|https://open.spot...|      |https://api.spoti...|https://i.scdn.co...|Empire Mates Ente...|                 Ayo|        61|                 day|  2014-09-17|        null|         19|album|spotify:album:3K4...|6742034f1f8786457...|\n",
      "|16xW2AvG6yVXJJ0ZY...|Wizkid|3tVQdUvClmAT7URs9...|AR,AU,AT,BE,BO,BR...|2022 Empire Mates...|    2024-11-23|https://open.spot...|      |https://api.spoti...|https://i.scdn.co...|Empire Mates Ente...|           Superstar|        57|                 day|  2011-06-12|        null|         17|album|spotify:album:16x...|6742034f1f8786457...|\n",
      "+--------------------+------+--------------------+--------------------+--------------------+--------------+--------------------+------+--------------------+--------------------+--------------------+--------------------+----------+--------------------+------------+------------+-----------+-----+--------------------+--------------------+\n",
      "\n",
      "Starting to read data from database 'music_database' and collection 'trackfeature_collection'...\n",
      "+--------------------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+--------------------+--------------+--------------------+--------------+\n",
      "|            Track_ID|Danceability|Energy|Key|Loudness|Mode|Speechiness|Acousticness|Instrumentalness|Liveness|Valence|  Tempo|Time_signature|          Track_href|  Type_Feature|        Analysis_Url|Execution_date|\n",
      "+--------------------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+--------------------+--------------+--------------------+--------------+\n",
      "|2gVhfX2Gy1T9kDuS9...|       0.392| 0.574|  7|  -9.195|   1|       0.17|       0.833|         0.00179|   0.145|  0.529| 81.112|             4|https://api.spoti...|audio_features|https://api.spoti...|    2024-11-21|\n",
      "|1gcyHQpBQ1lfXGdhZ...|       0.462|  0.24|  0| -12.077|   1|     0.0377|        0.92|             0.0|   0.113|   0.32|171.319|             4|https://api.spoti...|audio_features|https://api.spoti...|    2024-11-21|\n",
      "|3Dby3p1m6IOZn2gII...|       0.512| 0.462|  9| -10.491|   1|     0.0408|        0.83|           0.166|   0.121|  0.353| 112.05|             4|https://api.spoti...|audio_features|https://api.spoti...|    2024-11-21|\n",
      "|6sQckd3Z8NPxVVKUn...|       0.575| 0.434|  5|  -8.193|   1|     0.0312|       0.735|         6.59E-5|   0.105|  0.348|145.916|             4|https://api.spoti...|audio_features|https://api.spoti...|    2024-11-21|\n",
      "|6lCvK2AR2uOKkVFCV...|       0.316| 0.361|  9| -10.381|   1|     0.0488|       0.878|         2.67E-5|  0.0797|  0.221| 74.952|             5|https://api.spoti...|audio_features|https://api.spoti...|    2024-11-21|\n",
      "|6uwfVkaOM1mcMkFmS...|       0.546| 0.613|  7|  -7.589|   0|     0.0264|       0.418|             0.0|   0.103|  0.535| 79.015|             4|https://api.spoti...|audio_features|https://api.spoti...|    2024-11-21|\n",
      "|55Vf4bimc1Rtfg0PA...|       0.559| 0.334| 11| -10.733|   1|     0.0376|        0.87|             0.0|   0.114|  0.211|122.079|             4|https://api.spoti...|audio_features|https://api.spoti...|    2024-11-21|\n",
      "|66tOfHVH3aUrscg8v...|       0.605| 0.488|  4|  -8.322|   1|     0.0264|       0.696|             0.0|   0.129|  0.354|119.966|             4|https://api.spoti...|audio_features|https://api.spoti...|    2024-11-21|\n",
      "|2awNGIJHodfLZSClB...|       0.537| 0.537|  8| -11.266|   1|     0.0617|       0.819|         9.04E-4|   0.142|  0.292|107.895|             4|https://api.spoti...|audio_features|https://api.spoti...|    2024-11-21|\n",
      "|43Ykum9T72UOPhBN3...|       0.515| 0.545|  2|  -9.277|   1|     0.0353|       0.855|         1.97E-5|  0.0921|  0.535| 88.856|             4|https://api.spoti...|audio_features|https://api.spoti...|    2024-11-21|\n",
      "|52OkpDsU6MmPx1AwG...|       0.604| 0.517|  0|  -9.014|   1|     0.0347|       0.768|         1.55E-4|   0.123|  0.511|127.967|             4|https://api.spoti...|audio_features|https://api.spoti...|    2024-11-21|\n",
      "|5VYWxXUpxuxEmCqML...|       0.546|  0.73|  0|  -7.704|   1|     0.0417|        0.66|           0.179|  0.0972|  0.573|157.895|             4|https://api.spoti...|audio_features|https://api.spoti...|    2024-11-21|\n",
      "|5uICWmZTLkpEVbK22...|       0.535| 0.561|  5| -11.609|   1|     0.0484|       0.876|         1.36E-4|   0.115|  0.287| 96.103|             4|https://api.spoti...|audio_features|https://api.spoti...|    2024-11-21|\n",
      "|6a8aUhYbaQBUI8PcJ...|       0.689| 0.704|  9| -10.813|   1|      0.245|       0.835|         4.83E-6|   0.134|   0.92|151.884|             5|https://api.spoti...|audio_features|https://api.spoti...|    2024-11-21|\n",
      "|6Wlq9rqkxrqj5Kls4...|        0.39|  0.27|  1| -10.673|   1|     0.0308|       0.937|         0.00227|   0.111|   0.32|125.177|             5|https://api.spoti...|audio_features|https://api.spoti...|    2024-11-21|\n",
      "|3zwMVvkBe2qIKDObW...|       0.581| 0.619| 10|  -6.524|   1|     0.0282|       0.769|         1.06E-4|   0.117|  0.645|137.915|             4|https://api.spoti...|audio_features|https://api.spoti...|    2024-11-21|\n",
      "|1kdWw77ZpYOkhxeuh...|       0.592|  0.41|  0| -12.426|   1|     0.0397|       0.801|         1.39E-5|    0.09|  0.416|151.923|             4|https://api.spoti...|audio_features|https://api.spoti...|    2024-11-21|\n",
      "|0lx2cLdOt3piJbcaX...|       0.392| 0.574|  7|  -9.195|   1|       0.17|       0.833|         0.00179|   0.145|  0.529| 81.112|             4|https://api.spoti...|audio_features|https://api.spoti...|    2024-11-21|\n",
      "|0sY6ZUTh4yoctD8VI...|       0.462|  0.24|  0| -12.077|   1|     0.0377|        0.92|             0.0|   0.113|   0.32|171.319|             4|https://api.spoti...|audio_features|https://api.spoti...|    2024-11-21|\n",
      "|5BK0uqwY9DNfZ630S...|       0.512| 0.462|  9| -10.491|   1|     0.0408|        0.83|           0.166|   0.121|  0.353| 112.05|             4|https://api.spoti...|audio_features|https://api.spoti...|    2024-11-21|\n",
      "+--------------------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+--------------------+--------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Successfully stopped Spark Session!\n",
      "------------------------------ Bronze task finished! -------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, col, get_json_object, to_date, regexp_replace, length, to_date\n",
    "import argparse\n",
    "\n",
    "\"\"\" Applying schemas and loading data from MongoDB into HDFS.\"\"\"\n",
    "def bronze_task(Execution_date: str):\n",
    "    #get spark Session\n",
    "    with get_sparkSession(appName = 'Bronze_task') as spark:\n",
    "        \"\"\"------------------------ BRONZE ARTIST ------------------------\"\"\"\n",
    "        artist_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'artist_collection')\n",
    "        artist_data = artist_data.filter(artist_data['Execution_date'] == Execution_date)\n",
    "        artist_data.show()\n",
    "        print(\"Starting bronze preprocessing for artist data...\")\n",
    "        #preprocessing before loading data\n",
    "        try:\n",
    "            artist_data = artist_data.withColumn('Genres', split(col('Genres'), \",\")) \\\n",
    "                                    .withColumn('Followers', col('Followers').cast('int')) \\\n",
    "                                    .withColumn('Popularity', col('Popularity').cast('int')) \\\n",
    "                                    .withColumn('External_Url', get_json_object(col('External_Url'),'$.spotify')) \\\n",
    "                                    .withColumn('Execution_date', col('Execution_date').cast('date'))\n",
    "                                    \n",
    "            #reorder columns after reading \n",
    "            artist_data = artist_data.select('Artist_ID', 'Artist_Name', 'Genres', \n",
    "                                            'Followers', 'Popularity', 'Artist_Image', \n",
    "                                            'Artist_Type', 'External_Url', 'Href', 'Artist_Uri', 'Execution_date')\n",
    "            #applying schema        \n",
    "            artist_data = spark.createDataFrame(artist_data.rdd, schema = get_schema('artist'))\n",
    "            print(artist_data)\n",
    "            print(\"Finished bronze preprocessing for artist data.\")\n",
    "\n",
    "            #upload data into HDFS\n",
    "            write_HDFS(spark, data = artist_data, direct = 'bronze_data/bronze_artist', file_type = 'parquet')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while preprocessing bronze data: {e}\")\n",
    "\n",
    "        # \"\"\"------------------------ BRONE ALBUM ------------------------\"\"\"\n",
    "        album_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'album_collection')\n",
    "        album_data = album_data.filter(album_data['Execution_date'] == Execution_date)\n",
    "        album_data.show()\n",
    "        # print(\"Starting bronze preprocessing for album data...\")\n",
    "        # try:\n",
    "        #     album_data = album_data.withColumn('Popularity', col('Popularity').cast('int')) \\\n",
    "        #                         .withColumn('Release_Date', to_date('Release_Date', \"MM/dd/yyyy\")) \\\n",
    "        #                         .withColumn('TotalTracks', col('TotalTracks').cast('int'))\n",
    "        #     #reorder columns after reading\n",
    "        #     album_data = album_data.select('Artist', 'Artist_ID', 'Album_ID', 'Name', 'Type', 'Genres', \n",
    "        #                                 'Label', 'Popularity', 'Available_Markets', 'Release_Date', \n",
    "        #                                 'ReleaseDatePrecision', 'TotalTracks', 'Copyrights', 'Restrictions', \n",
    "        #                                 'External_URL', 'Href', 'Image', 'Uri', 'Execution_date')\n",
    "        #     album_data = spark.createDataFrame(album_data.rdd, schema = get_schema('album'))\n",
    "        #     print(\"Finished bronze preprocessing for album data.\")\n",
    "        #     #upload data into HDFS\n",
    "        #     write_HDFS(spark, data = album_data, direct = 'bronze_data/bronze_album', file_type = 'parquet')\n",
    "        # except Exception as e:\n",
    "        #     print(f\"An error occurred while preprocessing bronze data: {e}\")\n",
    "\n",
    "\n",
    "        \"\"\"------------------------ BRONZE TRACK -------------------------\"\"\"\n",
    "        #track_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'track_collection', schema = get_schema('track'))\n",
    "        #track_data = track_data.filter(track_data['Execution_date'] == Execution_date)\n",
    "        #track_data.show()\n",
    "        #upload data into HDFS\n",
    "        # write_HDFS(spark, data = track_data, direct = 'bronze_data/bronze_track', file_type = 'parquet')\n",
    "\n",
    "\n",
    "        # \"\"\"------------------------ BRONZE TRACK FEATURE ------------------------\"\"\"\n",
    "        track_feature_data = read_mongoDB(spark, database_name = 'music_database', collection_name = 'trackfeature_collection', \n",
    "                                          schema = get_schema('trackfeature'))\n",
    "        track_feature_data = track_feature_data.withColumn('Execution_date', col('Execution_date').cast(\"date\"))\n",
    "        track_feature_data.show()\n",
    "        # track_feature_data = track_feature_data.filter(track_feature_data['Execution_date'] == Execution_date)\n",
    "        # #upload data into HDFS\n",
    "        # write_HDFS(spark, data = track_feature_data, direct = 'bronze_data/bronze_track_feature', file_type = 'parquet')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # parser = argparse.ArgumentParser(description = \"Current date argument\")\n",
    "    # parser.add_argument('--execution_date', required = False, help = \"data for execution_date\")\n",
    "    # args = parser.parse_args()\n",
    "    print(\"------------------------------- Bronze task starts! -------------------------------\")\n",
    "    bronze_task(\"2024-11-23\")\n",
    "    print(\"------------------------------ Bronze task finished! -------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import explode_outer, ltrim\n",
    "\n",
    "\"\"\" Create SilverLayer class to process data in the Silver layer. \"\"\"\n",
    "class SilverLayer:\n",
    "    #init \n",
    "    def __init__(self, data: pyspark.sql.DataFrame, \n",
    "                 drop_columns: list = None, \n",
    "                 drop_null_columns: list = None,\n",
    "                 fill_nulls_columns: dict = None,\n",
    "                 duplicate_columns: list = None,\n",
    "                 nested_columns: list = None,\n",
    "                 rename_columns: dict = None,\n",
    "                 ):\n",
    "        \n",
    "        #check valid params\n",
    "        if data is not None and not isinstance(data, pyspark.sql.DataFrame):\n",
    "            raise TypeError(\"data must be a DataFrame!\")\n",
    "        \n",
    "        if drop_columns is not None and not isinstance(drop_columns, list):\n",
    "            raise TypeError(\"drop_columns must be a list!\")\n",
    "        \n",
    "        if drop_null_columns is not None and not isinstance(drop_null_columns, list):\n",
    "            raise TypeError(\"drop_null_columns must be a list!\")\n",
    "        \n",
    "        if fill_nulls_columns is not None and not isinstance(fill_nulls_columns, dict):\n",
    "            raise TypeError(\"handle_nulls must be a dict!\")\n",
    "        \n",
    "        if duplicate_columns is not None and not isinstance(duplicate_columns, list):\n",
    "            raise TypeError(\"duplicate_columns must be a list!\")\n",
    "        \n",
    "        if nested_columns is not None and not isinstance(nested_columns, list):\n",
    "            raise TypeError(\"handle_nested must be a list!\")\n",
    "        \n",
    "        if rename_columns is not None and not isinstance(rename_columns, dict):\n",
    "            raise TypeError(\"rename_columns must be a dict!\")\n",
    "        \"\"\"Initialize class attributes for data processing.\"\"\"\n",
    "        self._data = data\n",
    "        self._drop_columns = drop_columns\n",
    "        self._drop_null_columns = drop_null_columns\n",
    "        self._fill_nulls_columns = fill_nulls_columns\n",
    "        self._duplicate_columns = duplicate_columns\n",
    "        self._nested_columns = nested_columns\n",
    "        self._rename_columns = rename_columns\n",
    "\n",
    "\n",
    "    \"\"\" Method to drop unnecessary columns. \"\"\"\n",
    "    def drop(self):\n",
    "        self._data = self._data.drop(*self._drop_columns)\n",
    "\n",
    "    \n",
    "    \"\"\" Method to drop rows based on null values in each column. \"\"\"\n",
    "    def drop_null(self):\n",
    "        self._data = self._data.dropna(subset = self._drop_null_columns, how = \"all\")\n",
    "\n",
    "    \n",
    "    \"\"\" Method to fill null values. \"\"\"\n",
    "    def fill_null(self):\n",
    "        for column_list, value in self._fill_nulls_columns.items():\n",
    "            self._data = self._data.fillna(value = value, subset = column_list)\n",
    "\n",
    "\n",
    "    \"\"\" Method to rename columns. \"\"\"\n",
    "    def rename(self):\n",
    "        for old_name, new_name in self._rename_columns.items():\n",
    "            self._data = self._data.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "    \"\"\" Method to handle duplicates. \"\"\"\n",
    "    def handle_duplicate(self):\n",
    "        self._data = self._data.dropDuplicates(self._duplicate_columns)\n",
    "\n",
    "    \"\"\" Method to handle nested. \"\"\"\n",
    "    def handle_nested(self):\n",
    "        for column in self._nested_columns:\n",
    "            self._data = self._data.withColumn(column, explode_outer(column)) \\\n",
    "                                   .withColumn(column, ltrim(column))\n",
    "    \n",
    "    \"\"\" Main processing. \"\"\"\n",
    "    def process(self) -> pyspark.sql.DataFrame:\n",
    "        #drop unnecessary columns\n",
    "        if self._drop_columns:\n",
    "            self.drop() \n",
    "\n",
    "        #drop rows contain null values for each col\n",
    "        if self._drop_null_columns:\n",
    "            self.drop_null()\n",
    "\n",
    "        #fill null values\n",
    "        if self._fill_nulls_columns:\n",
    "            self.fill_null()\n",
    "        \n",
    "        #handle duplicate rows\n",
    "        if self._duplicate_columns:\n",
    "            self.handle_duplicate()\n",
    "\n",
    "        #handle nested columns \n",
    "        if self._nested_columns:\n",
    "            self.handle_nested()\n",
    "\n",
    "        #rename columns\n",
    "        if self._rename_columns:\n",
    "            self.rename()\n",
    "\n",
    "        return self._data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Spark Session with app name: silver_task and master: local!\n",
      "Starting to read data from hdfs://namenode:9000/datalake/bronze_data/bronze_artist...\n",
      "Processing for 'silver_artist' ...\n",
      "Finished processing for 'silver_artist'.\n",
      "Starting to upload 'silver_artist' into hdfs://namenode:9000/datalake/silver_data/silver_artist...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'silver_artist' into HDFS.\n",
      "Starting to read data from hdfs://namenode:9000/datalake/bronze_data/bronze_album...\n",
      "Processing for 'silver_album' ...\n",
      "Finished processing for 'silver_album'.\n",
      "Starting to upload 'silver_album' into hdfs://namenode:9000/datalake/silver_data/silver_album...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 06:51:01 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'silver_album' into HDFS.\n",
      "Starting to read data from hdfs://namenode:9000/datalake/bronze_data/bronze_track...\n",
      "Processing for 'silver_track' ...\n",
      "Finished processing for 'silver_track'.\n",
      "Starting to upload 'silver_track' into hdfs://namenode:9000/datalake/silver_data/silver_track...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'silver_track' into HDFS.\n",
      "Starting to read data from hdfs://namenode:9000/datalake/bronze_data/bronze_track_feature...\n",
      "Processing for 'silver_track_feature' ...\n",
      "Finished processing for 'silver_track_feature'.\n",
      "Starting to upload 'silver_track_feature' into hdfs://namenode:9000/datalake/silver_data/silver_track_feature...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'silver_track_feature' into HDFS.\n",
      "Successfully stopped Spark Session!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, year\n",
    "\n",
    "\"\"\" Processing silver artist data. \"\"\"\n",
    "def silver_artist_process(spark):\n",
    "    #read bronze artist data\n",
    "    bronze_artist = read_HDFS(spark, HDFS_dir = \"bronze_data/bronze_artist\", file_type = 'parquet')\n",
    "    #applying SilverLayer class \n",
    "    silver_artist = SilverLayer(data = bronze_artist, \n",
    "                                drop_columns       = ['Artist_Type', 'Href', 'Artist_Uri'],\n",
    "                                drop_null_columns  = ['Artist_ID'], \n",
    "                                fill_nulls_columns = {'Followers': 0,\n",
    "                                                      'Popularity': 0},\n",
    "                                duplicate_columns  = ['Artist_ID'],\n",
    "                                nested_columns     = ['Genres'],\n",
    "                                rename_columns     = {'Artist_ID': 'id',\n",
    "                                                      'Artist_Name': 'name',\n",
    "                                                      'Genres': 'genres',\n",
    "                                                      'Followers': 'followers',\n",
    "                                                      'Popularity': 'popularity',\n",
    "                                                      'Artist_Image': 'link_image',\n",
    "                                                      'External_Url': 'url'})\n",
    "    \n",
    "    #processing data\n",
    "    print(\"Processing for 'silver_artist' ...\")\n",
    "    silver_artist = silver_artist.process()\n",
    "    print(\"Finished processing for 'silver_artist'.\")\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = silver_artist, direct = \"silver_data/silver_artist\", file_type = 'parquet')\n",
    "\n",
    "\n",
    "\"\"\" Processing silver album data. \"\"\"\n",
    "def silver_album_process(spark):\n",
    "    #read bronze album data\n",
    "    bronze_album = read_HDFS(spark, HDFS_dir = 'bronze_data/bronze_album', file_type = 'parquet')\n",
    "    #applying Silver Layer class\n",
    "    silver_album = SilverLayer(data = bronze_album,\n",
    "                               drop_columns       = ['Genres', 'Available_Markets', 'Restrictions', 'Href','Uri'],\n",
    "                               drop_null_columns  = ['Album_ID'],\n",
    "                               fill_nulls_columns = {'Popularity': 0,\n",
    "                                                     'TotalTracks': 0},\n",
    "                               duplicate_columns  = ['Album_ID'],\n",
    "                               rename_columns     = {'Artist': 'artist',\n",
    "                                                     'Artist_ID': 'artist_id',\n",
    "                                                     'Album_ID': 'id',\n",
    "                                                     'Name': 'name',\n",
    "                                                     'Type': 'type',\n",
    "                                                     'Label': 'label',\n",
    "                                                     'Popularity': 'popularity',\n",
    "                                                     'Release_Date': 'release_date',\n",
    "                                                     'ReleaseDatePrecision': 'release_date_precision',\n",
    "                                                     'TotalTracks': 'total_tracks',\n",
    "                                                     'Copyrights': 'copyrights',\n",
    "                                                     'External_URL': 'url',\n",
    "                                                     'Image': 'link_image'})\n",
    "    \n",
    "    #processing data\n",
    "    print(\"Processing for 'silver_album' ...\")\n",
    "    silver_album = silver_album.process()\n",
    "    print(\"Finished processing for 'silver_album'.\")\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = silver_album, direct = 'silver_data/silver_album', file_type = 'parquet')\n",
    "\n",
    "\n",
    "\"\"\" Processing silver track data. \"\"\"\n",
    "def silver_track_process(spark):\n",
    "    #read bronze track data\n",
    "    bronze_track = read_HDFS(spark, HDFS_dir = 'bronze_data/bronze_track', file_type = 'parquet')\n",
    "    #applying Silver Layer class\n",
    "    silver_track = SilverLayer(data               = bronze_track,\n",
    "                               drop_columns       = ['Artists', 'Type', 'AvailableMarkets', 'Href', 'Uri', 'Is_Local'],\n",
    "                               drop_null_columns  = ['Track_ID'],\n",
    "                               fill_nulls_columns = {'Restrictions': 'None'},\n",
    "                               duplicate_columns  = ['Track_ID'],\n",
    "                               rename_columns     = {'Album_ID': 'album_id',\n",
    "                                                     'Album_Name': 'album_name',\n",
    "                                                     'Track_ID': 'id',\n",
    "                                                     'Name': 'name',\n",
    "                                                     'Track_Number': 'track_number',\n",
    "                                                     'Disc_Number': 'disc_number',\n",
    "                                                     'Duration_ms': 'duration_ms',\n",
    "                                                     'Explicit': 'explicit',\n",
    "                                                     'External_urls': 'url',\n",
    "                                                     'Restrictions': 'restriction',\n",
    "                                                     'Preview_url': 'preview'})\n",
    "    \n",
    "    #processing data\n",
    "    print(\"Processing for 'silver_track' ...\")\n",
    "    silver_track = silver_track.process()\n",
    "    print(\"Finished processing for 'silver_track'.\")\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = silver_track, direct = 'silver_data/silver_track', file_type = 'parquet')\n",
    "\n",
    "\n",
    "\"\"\" Processing silver track feature data. \"\"\"\n",
    "def silver_track_feature_process(spark):\n",
    "    #read silver track feature data\n",
    "    bronze_track_feature = read_HDFS(spark, HDFS_dir = 'bronze_data/bronze_track_feature', file_type = 'parquet')\n",
    "    #applying Silver Layer class\n",
    "    silver_track_feature = SilverLayer(data              = bronze_track_feature,\n",
    "                                       drop_columns      = ['Track_href', 'Type_Feature', 'Analysis_Url'],\n",
    "                                       drop_null_columns = ['Track_ID'],\n",
    "                                       duplicate_columns = ['Track_ID'],\n",
    "                                       rename_columns    = {'Track_ID': 'id',\n",
    "                                                            'Danceability': 'danceability',\n",
    "                                                            'Energy': 'energy',\n",
    "                                                            'Key': 'key',\n",
    "                                                            'Loudness': 'loudness',\n",
    "                                                            'Mode': 'mode',\n",
    "                                                            'Speechiness': 'speechiness',\n",
    "                                                            'Acousticness': 'acousticness',\n",
    "                                                            'Instrumentalness': 'instrumentalness',\n",
    "                                                            'Liveness': 'liveness',\n",
    "                                                            'Valence': 'valence',\n",
    "                                                            'Tempo': 'tempo',\n",
    "                                                            'Time_signature': 'time_signature'})\n",
    "    #processing data\n",
    "    print(\"Processing for 'silver_track_feature' ...\")\n",
    "    silver_track_feature = silver_track_feature.process()\n",
    "    print(\"Finished processing for 'silver_track_feature'.\")\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = silver_track_feature, direct = 'silver_data/silver_track_feature', file_type = 'parquet')\n",
    "\n",
    "\n",
    "#main call\n",
    "if __name__ == \"__main__\":\n",
    "    with get_sparkSession(\"silver_task_spark\") as spark:\n",
    "        print(\"------------------------------- Silver task starts! -------------------------------\")\n",
    "        print(\"Starting silver artist data processing...\")\n",
    "        silver_artist_process(spark)\n",
    "        print(\"Starting silver album data processing...\")\n",
    "        silver_album_process(spark)\n",
    "        print(\"Starting silver track data processing...\")\n",
    "        silver_track_process(spark)\n",
    "        print(\"Starting silver track feature data processing...\")\n",
    "        silver_track_feature_process(spark)\n",
    "        print(\"------------------------------ Silver task finished! -------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-11c22dda-9466-49ea-8d07-3419263643cb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.4.0 in central\n",
      "You probably access the destination server through a proxy server that is not well configured.\n",
      "You probably access the destination server through a proxy server that is not well configured.\n",
      "You probably access the destination server through a proxy server that is not well configured.\n",
      "You probably access the destination server through a proxy server that is not well configured.\n",
      ":: resolution report :: resolve 58628ms :: artifacts dl 18ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.4.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   1   |   1   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: WARNINGS\n",
      "\tproblem while downloading module descriptor: https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/5.1.4/mongodb-driver-sync-5.1.4.pom: repo1.maven.org (22893ms)\n",
      "\n",
      "\tHost repos.spark-packages.org not found. url=https://repos.spark-packages.org/org/mongodb/mongodb-driver-sync/maven-metadata.xml\n",
      "\n",
      "\tHost repos.spark-packages.org not found. url=https://repos.spark-packages.org/org/mongodb/mongodb-driver-sync/\n",
      "\n",
      "\tHost repos.spark-packages.org not found. url=https://repos.spark-packages.org/org/mongodb/mongodb-driver-sync/\n",
      "\n",
      "\t\tmodule not found: org.mongodb#mongodb-driver-sync;[5.1.1,5.1.99)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/home/jovyan/.m2/repository/org/mongodb/mongodb-driver-sync/[revision]/mongodb-driver-sync-[revision].pom\n",
      "\n",
      "\t  -- artifact org.mongodb#mongodb-driver-sync;[5.1.1,5.1.99)!mongodb-driver-sync.jar:\n",
      "\n",
      "\t  file:/home/jovyan/.m2/repository/org/mongodb/mongodb-driver-sync/[revision]/mongodb-driver-sync-[revision].jar\n",
      "\n",
      "\t==== local-ivy-cache: tried\n",
      "\n",
      "\t  /home/jovyan/.ivy2/local/org.mongodb/mongodb-driver-sync/[revision]/ivys/ivy.xml\n",
      "\n",
      "\t  -- artifact org.mongodb#mongodb-driver-sync;[5.1.1,5.1.99)!mongodb-driver-sync.jar:\n",
      "\n",
      "\t  /home/jovyan/.ivy2/local/org.mongodb/mongodb-driver-sync/[revision]/jars/mongodb-driver-sync.jar\n",
      "\n",
      "\t==== central: tried\n",
      "\n",
      "\t  https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/[revision]/mongodb-driver-sync-[revision].pom\n",
      "\n",
      "\t==== spark-packages: tried\n",
      "\n",
      "\t  https://repos.spark-packages.org/org/mongodb/mongodb-driver-sync/[revision]/mongodb-driver-sync-[revision].pom\n",
      "\n",
      "\t  -- artifact org.mongodb#mongodb-driver-sync;[5.1.1,5.1.99)!mongodb-driver-sync.jar:\n",
      "\n",
      "\t  https://repos.spark-packages.org/org/mongodb/mongodb-driver-sync/[revision]/mongodb-driver-sync-[revision].jar\n",
      "\n",
      "\tHost repo1.maven.org not found. url=https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/10.4.0/mongo-spark-connector_2.12-10.4.0.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] org.mongodb.spark#mongo-spark-connector_2.12;10.4.0!mongo-spark-connector_2.12.jar (2ms)\n",
      "\n",
      "\t==== central: tried\n",
      "\n",
      "\t  https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/10.4.0/mongo-spark-connector_2.12-10.4.0.jar\n",
      "\n",
      "\t\t::::::::::::::::::::::::::::::::::::::::::::::\n",
      "\n",
      "\t\t::          UNRESOLVED DEPENDENCIES         ::\n",
      "\n",
      "\t\t::::::::::::::::::::::::::::::::::::::::::::::\n",
      "\n",
      "\t\t:: org.mongodb#mongodb-driver-sync;[5.1.1,5.1.99): not found\n",
      "\n",
      "\t\t::::::::::::::::::::::::::::::::::::::::::::::\n",
      "\n",
      "\n",
      "\t\t::::::::::::::::::::::::::::::::::::::::::::::\n",
      "\n",
      "\t\t::              FAILED DOWNLOADS            ::\n",
      "\n",
      "\t\t:: ^ see resolution messages for details  ^ ::\n",
      "\n",
      "\t\t::::::::::::::::::::::::::::::::::::::::::::::\n",
      "\n",
      "\t\t:: org.mongodb.spark#mongo-spark-connector_2.12;10.4.0!mongo-spark-connector_2.12.jar\n",
      "\n",
      "\t\t::::::::::::::::::::::::::::::::::::::::::::::\n",
      "\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      "Exception in thread \"main\" java.lang.RuntimeException: [unresolved dependency: org.mongodb#mongodb-driver-sync;[5.1.1,5.1.99): not found, download failed: org.mongodb.spark#mongo-spark-connector_2.12;10.4.0!mongo-spark-connector_2.12.jar]\n",
      "\tat org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:1599)\n",
      "\tat org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:185)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:332)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_sparkSession\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mspark\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mread_HDFS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHDFS_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgold_data/dim_artist\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExecution_date\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2024-11-29\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m, in \u001b[0;36mget_sparkSession\u001b[0;34m(appName, master)\u001b[0m\n\u001b[1;32m     16\u001b[0m conf \u001b[38;5;241m=\u001b[39m conf\u001b[38;5;241m.\u001b[39msetAppName(appName) \\\n\u001b[1;32m     17\u001b[0m            \u001b[38;5;241m.\u001b[39msetMaster(master) \\\n\u001b[1;32m     18\u001b[0m            \u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.executor.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4g\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m            \u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.legacy.timeParserPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEGACY\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     22\u001b[0m            \u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.jars.packages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.mongodb.spark:mongo-spark-connector_2.12:10.4.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m            \u001b[38;5;66;03m#.set(\"spark.jars.packages\", \"net.snowflake:spark-snowflake_2.12:2.12.0-spark_3.4\")\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#               .set(\"spark.executor.instances\", \"2\") \\\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#create Spark Session\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully created Spark Session with app name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mappName\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and master: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmaster\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#yield spark\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:477\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/context.py:512\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/context.py:198\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m     )\n\u001b[0;32m--> 198\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    201\u001b[0m         master,\n\u001b[1;32m    202\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    213\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/context.py:432\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 432\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/java_gateway.py:106\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    109\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "with get_sparkSession('test') as spark:\n",
    "    a = read_HDFS(spark, HDFS_dir = 'gold_data/dim_artist', file_type = 'parquet')\n",
    "    print(a.filter(a['Execution_date'] == \"2024-11-29\").count())\n",
    "    a.filter(a['Execution_date'] == \"2024-11-29\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Spark Session with app name: spark_for_gold_task and master: local!\n",
      "Starting to read data from hdfs://namenode:9000/datalake/silver_data/silver_artist...\n",
      "Starting to read data from hdfs://namenode:9000/datalake/silver_data/silver_album...\n",
      "Starting to read data from hdfs://namenode:9000/datalake/silver_data/silver_track...\n",
      "Starting to read data from hdfs://namenode:9000/datalake/silver_data/silver_track_feature...\n",
      "Starting to upload 'dim_genres' into hdfs://namenode:9000/datalake/gold_data/dim_genres...\n",
      "Successfully uploaded 'dim_genres' into HDFS.\n",
      "Starting to upload 'dim_artist' into hdfs://namenode:9000/datalake/gold_data/dim_artist...\n",
      "Successfully uploaded 'dim_artist' into HDFS.\n",
      "Starting to upload 'dim_artist_genres' into hdfs://namenode:9000/datalake/gold_data/dim_artist_genres...\n",
      "Successfully uploaded 'dim_artist_genres' into HDFS.\n",
      "Starting to upload 'dim_album' into hdfs://namenode:9000/datalake/gold_data/dim_album...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'dim_album' into HDFS.\n",
      "Starting to upload 'dim_track_feature' into hdfs://namenode:9000/datalake/gold_data/dim_track_feature...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'dim_track_feature' into HDFS.\n",
      "Starting to upload 'fact_track' into hdfs://namenode:9000/datalake/gold_data/fact_track...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'fact_track' into HDFS.\n",
      "Successfully stopped Spark Session!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\" Gold layer. \"\"\"\n",
    "#Handle table individually\n",
    "from pyspark.sql.functions import monotonically_increasing_id, concat, col, lit, count\n",
    "with get_sparkSession('spark_for_gold_task') as spark:\n",
    "    current_day = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    #Read data from HDFS\n",
    "    silver_artist = read_HDFS(spark, HDFS_dir = 'silver_data/silver_artist', file_type = 'parquet')\n",
    "    silver_album = read_HDFS(spark, HDFS_dir = 'silver_data/silver_album', file_type = 'parquet')\n",
    "    silver_track = read_HDFS(spark, HDFS_dir = 'silver_data/silver_track', file_type = 'parquet')\n",
    "    silver_track_feature = read_HDFS(spark, HDFS_dir = 'silver_data/silver_track_feature', file_type = 'parquet')\n",
    "\n",
    "\n",
    "    \"\"\" Create dim_genres table. \"\"\"\n",
    "    #list all distinct genres in artist table\n",
    "    dim_genres = silver_artist.select('genres', 'execution_date').distinct()\n",
    "    dim_genres = dim_genres.filter(col('genres').isNotNull())\n",
    "    #add primary key\n",
    "    dim_genres = dim_genres.withColumn(\"id\", monotonically_increasing_id()) \\\n",
    "                           .withColumn(\"id\", concat(lit(current_day.replace(\"-\", \"\")), col('id')))\n",
    "    #reorder columns\n",
    "    dim_genres = dim_genres.select(\"id\", \"genres\", \"execution_date\")\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = dim_genres, direct = 'gold_data/dim_genres', file_type = 'parquet')\n",
    "    \n",
    "\n",
    "    \"\"\" Create dim_artist table. \"\"\"\n",
    "    #just drop genres column and distinct row\n",
    "    dim_artist = silver_artist.drop('genres').distinct()\n",
    "    write_HDFS(spark, data = dim_artist, direct = 'gold_data/dim_artist', file_type = 'parquet')\n",
    "\n",
    "\n",
    "    \"\"\" Create dim_artist_genres table. \"\"\"\n",
    "    #select necessary columns in artist table\n",
    "    dim_artist_genres = silver_artist.select('id', 'genres') \\\n",
    "                                     .withColumnRenamed('id', 'artist_id')\n",
    "    #joining tables to map artist IDs and genre IDs\n",
    "    dim_artist_genres = dim_artist_genres.join(dim_genres, on = 'genres', how = 'inner') \\\n",
    "                                         .withColumnRenamed('id', 'genres_id')\n",
    "    #drop genres column\n",
    "    dim_artist_genres = dim_artist_genres.drop('genres')\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = dim_artist_genres, direct = 'gold_data/dim_artist_genres', file_type = 'parquet')\n",
    "\n",
    "\n",
    "    \"\"\" Create dim_album table. \"\"\"\n",
    "    #just drop unnecessary columns \n",
    "    dim_album = silver_album.drop('artist', 'artist_id', 'total_tracks', 'release_date_precision')\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = dim_album, direct = 'gold_data/dim_album', file_type = 'parquet')\n",
    "\n",
    "\n",
    "    \"\"\" Create dim_track_feature table. \"\"\"\n",
    "    #we don't need to do anything since the dim_track_feature table is complete\n",
    "    dim_track_feature = silver_track_feature\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = dim_track_feature, direct = 'gold_data/dim_track_feature', file_type = 'parquet')\n",
    "\n",
    "    \n",
    "    \"\"\" Create fact_track table. \"\"\"\n",
    "    #drop album name and rename track id column\n",
    "    fact_track = silver_track.drop('album_name') \\\n",
    "                             .withColumnRenamed('id', 'track_id')\n",
    "    #get artist ID from silver album table to create a foreign key for the fact_track table\n",
    "    silver_album = silver_album.select('id', 'artist_id') \\\n",
    "                               .withColumnRenamed('id', 'album_id')\n",
    "    fact_track = fact_track.join(silver_album, on = 'album_id', how = 'inner')\n",
    "    #reorder columns\n",
    "    fact_track = fact_track.select('track_id', 'artist_id', 'album_id', 'name', 'track_number', \n",
    "                                   'disc_number', 'duration_ms', 'explicit', 'url', 'restriction', 'preview', 'execution_date')\n",
    "    #load data into HDFS\n",
    "    write_HDFS(spark, data = fact_track, direct = 'gold_data/fact_track', file_type = 'parquet')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0ff282fc-a9fe-49ce-95eb-43570646f644;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.4.0 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;5.1.4 in central\n",
      "\t[5.1.4] org.mongodb#mongodb-driver-sync;[5.1.1,5.1.99)\n",
      "\tfound org.mongodb#bson;5.1.4 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;5.1.4 in central\n",
      "\tfound org.mongodb#bson-record-codec;5.1.4 in central\n",
      ":: resolution report :: resolve 13539ms :: artifacts dl 27ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;5.1.4 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;5.1.4 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.4.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   1   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0ff282fc-a9fe-49ce-95eb-43570646f644\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/28ms)\n",
      "24/11/21 13:21:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Spark Session with app name: test and master: local!\n",
      "Starting to read data from hdfs://namenode:9000/datalake/silver_data/silver_album...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+-----+--------------------+----------+------------+----------------------+------------+--------------------+--------------------+--------------------+--------------+\n",
      "|              artist|           artist_id|                  id|                name| type|               label|popularity|release_date|release_date_precision|total_tracks|          copyrights|                 url|          link_image|execution_date|\n",
      "+--------------------+--------------------+--------------------+--------------------+-----+--------------------+----------+------------+----------------------+------------+--------------------+--------------------+--------------------+--------------+\n",
      "|          Sonu Nigam|1dVygo6tRFXC8CSWU...|000LYwpRfBAlh4aFM...|Sonu Nigam Kannad...|album|        Lahari Music|        25|  2017-07-30|                   day|           7|2017 Lahari Recor...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|              AmaLee|4sf4DrAOkheqktxTy...|000fJX0XXUpWDpgit...|               Unity|album|    Leegion Creative|        42|  2020-08-05|                   day|          12|2020 Leegion Crea...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|     Mega Shinnosuke|72owWXEwmyfKq3ajr...|000hrcnG9LpofStj7...|         Culture Dog|album|     Mega Shinnosuke|        19|  2021-09-08|                   day|          11|2021 Mega Shinnos...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|    Dayang Nurfaizah|1E5aZPein8p4Jf9zk...|000iB8Z8L2XmqQiW6...|                2007|album|New Southern Reco...|        14|  2007-06-18|                   day|          12|New Southern Reco...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|          The Motans|05qpk4JDcLSFNJSsP...|000mwAZN63acYRoma...|  Great Expectations|album|      Global Records|        44|  2022-05-13|                   day|          12|2022 Global Recor...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|     Robbie Williams|2HcwFjNelS49kFbfv...|002H8sA77XFikjH4k...|The Heavy Enterta...|album|            Columbia|        52|  2016-11-04|                   day|          16|(P) 2016 Robert W...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|             CAKEBOY|6qEwx66sEhRl98CXP...|002f2iCucCNnMwVgL...|             LUVANDA|album|         Koala Music|        23|  2020-03-27|                   day|          11|2020 GLAM GO GANG...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|Sons de la Nature...|5n65kD8k3MMdHUTIz...|003W4yNNPuYiI9c0Q...|Bruit de la pluie...|album|      Integral Music|        37|  2023-02-27|                   day|         100|2023 Integral Mus...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|  Paloma San Basilio|1QcwtcwAClkmeaVof...|0040c1kULS1ykbMFk...|            Escorpio|album|    Orosound Records|         6|  2013-11-18|                   day|          11|(C) 2013 Orosound...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|               Don Q|5TM9R6dNoJSMq23yZ...|0046G1euhbWOAtnwK...|            Corleone|album|Highbridge the La...|        30|  2022-06-24|                   day|          11|2022 Highbridge t...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|      Camila Cabello|4nDoRrQiYLoBzwC5B...|004ywPlW72Hgn1Bo9...|              C,XOXO|album|Geffen/Interscope...|        73|  2024-06-28|                   day|          14| 2024 Camila Cab...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|Newborn Sleep Mus...|2vpce9EltQyoH9lVn...|005E8jJyWA1aKhMIe...|Moonlit Meadows: ...|album|Merry Measures St...|         0|  2024-09-23|                   day|          30|2024 Merry Measur...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|  Brass Construction|5jWuVnPLmjABrgCGi...|005moGTm1TocUHCTN...|Conquest (Expande...|album|CAPITOL CATALOG M...|         4|  1985-01-01|                   day|          12| 1985 Capitol Re...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|   Roberto Vecchioni|3TVifQ5FPcIzzcYSU...|005w0J5X9b2u9bwtv...|  Il Cielo Capovolto|album|       EMI Marketing|        28|  1995-01-01|                   day|          10| 1995 EMI Music ...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|           21 Savage|1URnnhqYAYcrqrcwq...|007DWn799UWvfY1ww...|        i am > i was|album|Slaughter Gang, L...|        79|  2018-12-21|                   day|          15|(P) 2018 Slaughte...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|           Nightwish|2NPduAUeLVsfIauhR...|007KuGgJRyLlPvIkG...|Decades: Live in ...|album|       Nuclear Blast|        41|  2019-12-06|                   day|          21|2019 Nuclear Blas...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|         David Broza|3BCJyAgxvYyeIjQyo...|007ZwjOEezTooZWqo...|  ...|album| David Broza Records|        16|  1994-01-01|                   day|          13|(C) 2014 David Br...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|         Bjarni mar|30NBEBMzDmnnw5lvs...|008WvCl6UF4JMiDyf...|           Fyrirheit|album|Bjarni mar Haral...|        29|  2016-05-21|                   day|          12|2016 Bjarni mar ...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|Konstantin Klasht...|6OzDh0bCqcwWZfGNV...|0097npz4ZQXbUcMDo...|      Smooth Jazz IV|album|           Kvk Music|        14|  2017-06-02|                   day|          10|2017 Konstantin K...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "|          VDJ WIZKEL|08L39ZVoopuNLrODG...|009ULF23b9DugiC3c...| Yan Yan Always Hard|album|          VDJ WIZKEL|        17|  2024-02-28|                   day|           4|2024 VDJ WIZKEL, ...|https://open.spot...|https://i.scdn.co...|    2024-11-19|\n",
      "+--------------------+--------------------+--------------------+--------------------+-----+--------------------+----------+------------+----------------------+------------+--------------------+--------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "67544\n",
      "Successfully stopped Spark Session!\n"
     ]
    }
   ],
   "source": [
    "with get_sparkSession('test') as spark:\n",
    "    data = read_HDFS(spark, HDFS_dir = \"silver_data/silver_album\", file_type = 'parquet')dt.\n",
    "    data.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SnowFlake loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "net.snowflake#spark-snowflake_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-12153588-065c-41bc-ac39-e3b008a09fdc;1.0\n",
      "\tconfs: [default]\n",
      "\tfound net.snowflake#spark-snowflake_2.12;2.12.0-spark_3.4 in central\n",
      "\tfound net.snowflake#snowflake-ingest-sdk;0.10.8 in central\n",
      "\tfound net.snowflake#snowflake-jdbc;3.13.30 in central\n",
      ":: resolution report :: resolve 485ms :: artifacts dl 17ms\n",
      "\t:: modules in use:\n",
      "\tnet.snowflake#snowflake-ingest-sdk;0.10.8 from central in [default]\n",
      "\tnet.snowflake#snowflake-jdbc;3.13.30 from central in [default]\n",
      "\tnet.snowflake#spark-snowflake_2.12;2.12.0-spark_3.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-12153588-065c-41bc-ac39-e3b008a09fdc\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/11ms)\n",
      "24/11/19 13:22:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Spark Session with app name: snowflake_load_data_spark and master: local!\n",
      "Starting to read data from hdfs://namenode:9000/datalake/gold_data/dim_artist...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to upload dim_artist into SnowFlake...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.dim_artist' into SnowFlake.\n",
      "Starting to read data from hdfs://namenode:9000/datalake/gold_data/dim_genres...\n",
      "Starting to upload dim_genres into SnowFlake...\n",
      "Successfully uploaded 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.dim_genres' into SnowFlake.\n",
      "Starting to read data from hdfs://namenode:9000/datalake/gold_data/dim_artist_genres...\n",
      "Starting to upload dim_artist_genres into SnowFlake...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.dim_artist_genres' into SnowFlake.\n",
      "Starting to read data from hdfs://namenode:9000/datalake/gold_data/dim_album...\n",
      "Starting to upload dim_album into SnowFlake...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.dim_album' into SnowFlake.\n",
      "Starting to read data from hdfs://namenode:9000/datalake/gold_data/dim_track_feature...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to upload dim_track_feature into SnowFlake...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.dim_track_feature' into SnowFlake.\n",
      "Starting to read data from hdfs://namenode:9000/datalake/gold_data/fact_track...\n",
      "Starting to upload fact_track into SnowFlake...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.fact_track' into SnowFlake.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 13:24:41 WARN SparkConnectorContext$: Finish cancelling all queries for local-1732022544370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully stopped Spark Session!\n"
     ]
    }
   ],
   "source": [
    "def load_data_Snowflake(spark):\n",
    "    dim_artist = read_HDFS(spark, HDFS_dir = 'gold_data/dim_artist', file_type = 'parquet')\n",
    "    write_SnowFlake(spark, data = dim_artist, table_name = 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.dim_artist')\n",
    "    dim_genres = read_HDFS(spark, HDFS_dir = 'gold_data/dim_genres', file_type = 'parquet')\n",
    "    write_SnowFlake(spark, data = dim_genres, table_name = 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.dim_genres')\n",
    "    dim_artist_genres = read_HDFS(spark, HDFS_dir = 'gold_data/dim_artist_genres', file_type = 'parquet')\n",
    "    write_SnowFlake(spark, data = dim_artist_genres, table_name = 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.dim_artist_genres')\n",
    "    dim_album = read_HDFS(spark, HDFS_dir = 'gold_data/dim_album', file_type = 'parquet')\n",
    "    write_SnowFlake(spark, data= dim_album, table_name = 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.dim_album')\n",
    "    dim_track_feature = read_HDFS(spark, HDFS_dir = 'gold_data/dim_track_feature', file_type = 'parquet')\n",
    "    write_SnowFlake(spark, data = dim_track_feature, table_name = 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.dim_track_feature')\n",
    "    fact_track = read_HDFS(spark, HDFS_dir = 'gold_data/fact_track', file_type = 'parquet')\n",
    "    write_SnowFlake(spark, data = fact_track, table_name = 'SPOTIFY_MUSIC_DB.SPOTIFY_MUSIC_SCHEMA.fact_track')\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    with get_sparkSession(\"snowflake_load_data_spark\") as spark:\n",
    "        load_data_Snowflake(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Spark Session with app name: test and master: local!\n",
      "Starting to read data from hdfs://namenode:9000/datalake/silver_data/silver_artist...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---------+----------+--------------------+--------------------+--------------+\n",
      "|                  id|                name|              genres|followers|popularity|          link_image|                 url|Execution_date|\n",
      "+--------------------+--------------------+--------------------+---------+----------+--------------------+--------------------+--------------+\n",
      "|007FXgr0jLBJxhPJj...|           Eric Land|               forro|  1200389|        67|https://i.scdn.co...|https://open.spot...|    2024-11-30|\n",
      "|00GORbFeDmlAUD96S...|   Ancestral Rituals|                    |      445|        37|                null|https://open.spot...|    2024-11-30|\n",
      "|00IiVt687EdR9JnSo...|           Mazza_l20|              charva|    96013|        59|https://i.scdn.co...|https://open.spot...|    2024-11-30|\n",
      "|00IiVt687EdR9JnSo...|           Mazza_l20|          scouse rap|    96013|        59|https://i.scdn.co...|https://open.spot...|    2024-11-30|\n",
      "|010ZclHHy7BrTO5AQ...|             Hekje31|         dutch drill|    22177|        58|https://i.scdn.co...|https://open.spot...|    2024-11-30|\n",
      "|010ZclHHy7BrTO5AQ...|             Hekje31|       dutch hip hop|    22177|        58|https://i.scdn.co...|https://open.spot...|    2024-11-30|\n",
      "|01msMHTKcW8r2Y6eO...|         Goldi Yadav|        bhojpuri pop|     1411|        36|https://i.scdn.co...|https://open.spot...|    2024-11-30|\n",
      "|01nUdqwuIHCHLZNP2...|       Boy Wonder CF|      reggaeton flow|    89142|        65|https://i.scdn.co...|https://open.spot...|    2024-11-30|\n",
      "|04XdCDDrPnnqidaVB...|Hernan y La Champ...|            cuarteto|   133764|        60|https://i.scdn.co...|https://open.spot...|    2024-11-30|\n",
      "|04XdCDDrPnnqidaVB...|Hernan y La Champ...|          cumbia pop|   133764|        60|https://i.scdn.co...|https://open.spot...|    2024-11-30|\n",
      "|04XdCDDrPnnqidaVB...|Hernan y La Champ...|      cumbia villera|   133764|        60|https://i.scdn.co...|https://open.spot...|    2024-11-30|\n",
      "|06zaeAoWb4OP8f8ka...|    Jimmy Jitaraphol|         thai bl ost|    47819|        39|https://i.scdn.co...|https://open.spot...|    2024-11-30|\n",
      "|07DX1ovPdfcA492wX...|          Alles Kids|muziek voor kinderen|    12071|        65|https://i.scdn.co...|https://open.spot...|    2024-11-30|\n",
      "|07nrRL2MtV5V54nMs...|       Nader Al Atat|            arab pop|    44350|        33|https://i.scdn.co...|https://open.spot...|    2024-11-30|\n",
      "|07nrRL2MtV5V54nMs...|       Nader Al Atat|               dabke|    44350|        33|https://i.scdn.co...|https://open.spot...|    2024-11-30|\n",
      "|07nrRL2MtV5V54nMs...|       Nader Al Atat|        lebanese pop|    44350|        33|https://i.scdn.co...|https://open.spot...|    2024-11-30|\n",
      "|08WZUtFEM9Cx7z7gw...|   Katarina Zivkovic|          turbo folk|    48429|        31|https://i.scdn.co...|https://open.spot...|    2024-11-30|\n",
      "|08jywfUS0hp8XYlYs...|               Rampa|    deep disco house|   168590|        69|https://i.scdn.co...|https://open.spot...|    2024-11-30|\n",
      "|08jywfUS0hp8XYlYs...|               Rampa|       melodic house|   168590|        69|https://i.scdn.co...|https://open.spot...|    2024-11-30|\n",
      "|090DkzLApTeWVM3DO...|       Roza Zrgrli|                    |      963|        34|https://i.scdn.co...|https://open.spot...|    2024-11-30|\n",
      "+--------------------+--------------------+--------------------+---------+----------+--------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Successfully stopped Spark Session!\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `old_genres` cannot be resolved. Did you mean one of the following? [`id`, `name`, `genres`, `followers`, `popularity`, `link_image`, `url`, `Execution_date`].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m old_genres\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     12\u001b[0m new_genres \u001b[38;5;241m=\u001b[39m silver_artist\u001b[38;5;241m.\u001b[39mfilter(silver_artist[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExecution_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m Execution_date) \\\n\u001b[1;32m     13\u001b[0m                             \u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenres\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExecution_date\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m---> 15\u001b[0m new_genres \u001b[38;5;241m=\u001b[39m new_genres\u001b[38;5;241m.\u001b[39mjoin(old_genres, on \u001b[38;5;241m=\u001b[39m new_genres[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenres\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[43mold_genres\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mold_genres\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, how \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft_anti\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m dim_genres \u001b[38;5;241m=\u001b[39m new_genres\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenres\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExecution_date\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdistinct()\n\u001b[1;32m     18\u001b[0m dim_genres \u001b[38;5;241m=\u001b[39m dim_genres\u001b[38;5;241m.\u001b[39mfilter(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenres\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39misNotNull())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/dataframe.py:2930\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2858\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the column as a :class:`Column`.\u001b[39;00m\n\u001b[1;32m   2859\u001b[0m \n\u001b[1;32m   2860\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2927\u001b[0m \u001b[38;5;124;03m+---+----+\u001b[39;00m\n\u001b[1;32m   2928\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 2930\u001b[0m     jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2931\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n\u001b[1;32m   2932\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, Column):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `old_genres` cannot be resolved. Did you mean one of the following? [`id`, `name`, `genres`, `followers`, `popularity`, `link_image`, `url`, `Execution_date`]."
     ]
    }
   ],
   "source": [
    "Execution_date = \"2024-11-29\"\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, concat, lit\n",
    "with get_sparkSession(\"test\") as spark:\n",
    "    silver_artist = read_HDFS(spark, HDFS_dir = 'silver_data/silver_artist', file_type = 'parquet')\n",
    "\n",
    "    old_genres = silver_artist.filter(silver_artist['Execution_date'] != Execution_date) \\\n",
    "                                  .select('genres') \\\n",
    "                                  .withColumnRenamed('genres', 'old_genres') \n",
    "    #old_genres.show()\n",
    "    old_genres = silver_artist.filter(silver_artist['Execution_date'] != Execution_date)\n",
    "    old_genres.show()\n",
    "    new_genres = silver_artist.filter(silver_artist['Execution_date'] == Execution_date) \\\n",
    "                                .select('genres', 'Execution_date') \n",
    "                                \n",
    "    new_genres = new_genres.join(old_genres, on = new_genres['genres'] == old_genres['old_genres'], how = 'left_anti')\n",
    "\n",
    "    dim_genres = new_genres.select('genres', 'Execution_date').distinct()\n",
    "    dim_genres = dim_genres.filter(col('genres').isNotNull())\n",
    "    #add primary key\n",
    "    dim_genres = dim_genres.withColumn(\"id\", monotonically_increasing_id()) \\\n",
    "                            .withColumn(\"id\", concat(lit(Execution_date.replace(\"-\", \"\")), col('id')))\n",
    "    #reorder columns\n",
    "    dim_genres = dim_genres.select(\"id\", \"genres\", \"Execution_date\")\n",
    "    dim_genres.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting connect mongoDB...\n",
      "Client connected successfully!\n",
      "     Pos            Artist Execution_date\n",
      "126  127  Ovy On the Drums     2024-11-22\n",
      "Don't create the database 'music_database' because it already exists.\n",
      "Don't create the collection 'artist_name_collection' because it already exists.\n",
      "Successfully inserted data into collection 'artist_name_collection'.\n",
      "The connection to MongoDB has stopped!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # add pandas\n",
    "from datetime import datetime\n",
    "def crawl_new_artist_name():\n",
    "    # The URL of the website containing the art listing\n",
    "    url = 'https://kworb.net/itunes/extended.html'\n",
    "    # Read all tables from the website into a list\n",
    "    table = pd.read_html(url)\n",
    "    # Get the first table from the site data (The table includes the names of the artists)\n",
    "    ArtistName = table[0][['Pos','Artist']]\n",
    "    ArtistName['Execution_date'] = \"2024-11-22\"\n",
    "    return ArtistName.astype(str)\n",
    "    # Convert to csv file\n",
    "    # filePath = 'D:\\\\Study\\\\C++\\\\Source Code\\\\Python\\\\artistName.csv'\n",
    "    # ArtistName.to_csv(filePath,index=False)\n",
    "    print(\"Completed\")\n",
    "\n",
    "\n",
    "# crawl_new_artist_name()\n",
    "\n",
    "with mongoDB_client(username = 'huynhthuan', password = 'password') as client:\n",
    "    client_operations = mongoDB_operations(client)\n",
    "    old_artist_name_data = client_operations.read_data(database_name = 'music_database', collection_name = 'artist_name_collection')    \n",
    "    old_artist_name_data = old_artist_name_data[['Artist']]\n",
    "    old_artist_name_data.rename(columns = {'Artist': 'Old_Artist'}, inplace = True)\n",
    "    new_artist_name_data = crawl_new_artist_name()\n",
    "\n",
    "    daily_artist_name_data = pd.merge(old_artist_name_data, new_artist_name_data, left_on = 'Old_Artist', right_on = 'Artist', how = 'right')\n",
    "    daily_artist_name_data = daily_artist_name_data[daily_artist_name_data['Old_Artist'].isnull()][['Pos', 'Artist', 'Execution_date']]\n",
    "\n",
    "    daily_artist_name_data = daily_artist_name_data.head(1)\n",
    "\n",
    "    print(daily_artist_name_data)\n",
    "    \n",
    "    client_operations.insert_data(database_name = 'music_database', collection_name = 'artist_name_collection', data = daily_artist_name_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting connect mongoDB...\n",
      "Client connected successfully!\n",
      "                        _id  Pos            Artist Execution_date\n",
      "0  673ed65b1b1f107a33c932e2   27        Charli xcx     2024-11-22\n",
      "1  673ed6871b1f107a33c932e4  127  Ovy On the Drums     2024-11-22\n",
      "The connection to MongoDB has stopped!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from spotipy.exceptions import SpotifyException\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "#dfArtists = pd.read_csv('D:\\\\Study\\\\C++\\\\Source Code\\\\Python\\\\mydata.csv')\n",
    "#artist_file = 'D:\\\\Study\\\\C++\\\\Source Code\\\\Python\\\\artistInfo.csv'\n",
    "artist_columns=['Artist_ID', 'Artist_Name', 'Genres', 'Followers', 'Popularity', 'Artist_Image',\n",
    "                'Artist_Type', 'External_Url', 'Href', 'Artist_Uri']\n",
    "sp =spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id='48359fbc41d14367bb76ad67e4508f8e'\n",
    "                                                          ,client_secret='17b9ae9f31f44b82a7ce223e15c863bf'))\n",
    "def write_to_csv(file_path, data, columns):\n",
    "    # Chuyn data thnh dataframe vi cc ct  ch nh\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    # M file  ch  append ('a') v ghi d liu vo\n",
    "    with open(file_path, 'a', newline='', encoding='utf-8') as f:\n",
    "        # Ghi d liu vi c `header=False` nu file  tn ti, ch ghi header khi file trng\n",
    "        df.to_csv(f, header=f.tell() == 0, index=False)\n",
    "\n",
    "def getArtistData(artistName):\n",
    "    result = sp.search(q='artist:' + artistName, type='artist') #S dng bin api_call  lu li hm lambda vi li gi api tng ng\n",
    "    if result and result['artists']['items']:  # Kim tra nu tm thy ngh s\n",
    "        artist = result['artists']['items'][0]  # Ly ngh s u tin\n",
    "        artistId = artist['id']  # ID ngh s\n",
    "        artistInfo = { #Thm cc thng tin ca ngh s vo list \n",
    "            'name': artist['name'],\n",
    "            'genres': ', '.join(artist['genres']),  # Ni cc th loi li thnh chui\n",
    "            'followers': artist['followers']['total'],\n",
    "            'popularity':artist['popularity'],\n",
    "            'image':artist['images'][0]['url'] if artist['images'] else None,\n",
    "            'type':artist['type'],\n",
    "            'externalURL':artist['external_urls'],\n",
    "            'href':artist['href'],\n",
    "            'uri':artist['uri']\n",
    "    }\n",
    "        return artistId, artistInfo #Tr v ID v thng tin ca ngh s\n",
    "    print(f\"Can't find artist: {artistName}\")\n",
    "    return None,None #Tr v None nu khng c thng tin \n",
    "\n",
    "def start_to_crawl_artist(dfArtists: pd.DataFrame, Execution_date: str):\n",
    "    Artist_Data = [] \n",
    "    i=1\n",
    "    for artistName in dfArtists['Artist']: #Lp tng ngh s trong danh sch\n",
    "        print(str(i)+\")Loading Artist...\"+ artistName)\n",
    "        artistId,artistInfo = getArtistData(artistName) #Ly thng tin t hm  ci t\n",
    "        if artistId and artistInfo:\n",
    "            Artist_Data.append({ #Thm thng tin vo List lu tr\n",
    "                            'Artist_ID':artistId,\n",
    "                            'Artist_Name':artistInfo['name'],\n",
    "                            'Genres':artistInfo['genres'],\n",
    "                            'Followers':artistInfo['followers'],\n",
    "                            'Popularity':artistInfo['popularity'],\n",
    "                            'Artist_Image':artistInfo['image'],\n",
    "                            'Artist_Type':artistInfo['type'],\n",
    "                            'External_Url':artistInfo['externalURL'],\n",
    "                            'Href':artistInfo['href'],\n",
    "                            'Artist_Uri':artistInfo['uri']\n",
    "                    })\n",
    "            #write_to_csv(artist_file,Artist_Data,columns=artist_columns)\n",
    "        i+=1\n",
    "    Artist_Data = pd.DataFrame(Artist_Data)\n",
    "    Artist_Data['Execution_date'] = Execution_date\n",
    "    return Artist_Data\n",
    "    print(\"Successfully\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't read cache at: .cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting connect mongoDB...\n",
      "Client connected successfully!\n",
      "1)Loading Artist...Charli xcx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't write token to cache at: .cache\n",
      "Couldn't read cache at: .cache\n",
      "Couldn't write token to cache at: .cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2)Loading Artist...Ovy On the Drums\n",
      "Don't create the database 'music_database' because it already exists.\n",
      "Don't create the collection 'artist_collection' because it already exists.\n",
      "Successfully inserted data into collection 'artist_collection'.\n",
      "The connection to MongoDB has stopped!\n"
     ]
    }
   ],
   "source": [
    "with mongoDB_client(username = 'huynhthuan', password = 'password') as client:\n",
    "    client_operations = mongoDB_operations(client)\n",
    "    daily_artist_data = client_operations.read_data(database_name = 'music_database', collection_name = 'artist_name_collection', query = {'Execution_date': \"2024-11-22\"})\n",
    "    data = start_to_crawl_artist(daily_artist_data, Execution_date = \"2024-11-22\")\n",
    "    \n",
    "    client_operations.insert_data(database_name = 'music_database', collection_name = 'artist_collection', data = data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from spotipy.exceptions import SpotifyException\n",
    "import pandas as pd\n",
    "\n",
    "# Function to divide album list into small groups (chunks)\n",
    "def chunk_album_ids(album_ids,chunk_size=20):\n",
    "    for i in range(0,len(album_ids),chunk_size):\n",
    "        yield album_ids[i:i+chunk_size]\n",
    "        \n",
    "# Function to get album data from Spotify API\n",
    "def getAlbumData(artistId: pd.DataFrame, Execution_date: str):\n",
    "    sp =spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id='05e2ff0a21954615b11878a9eb038e7f'\n",
    "                                                          ,client_secret='7f2e7dc0bd0e41caa3665b5dea9ab8e0'))\n",
    "    \n",
    "    Album_Data =[]\n",
    "    Track_Data =[]\n",
    "    album_id_list = []\n",
    "    i=1\n",
    "\n",
    "    for artist in artistId['Artist_ID']:\n",
    "        result = sp.artist_albums(artist_id=artist,album_type ='album') #Get information from spotify and save it to result\n",
    "        if result and result['items']:\n",
    "            for album in result['items']: # Browse through each album to save to the list\n",
    "                album_id_list.append(album['id']) # Add album information to the list\n",
    "                \n",
    "        else:\n",
    "            print(f\"No albums found for artist ID: {artist}\")\n",
    "\n",
    "    #Split album list into chunks\n",
    "    for chunk in chunk_album_ids(album_id_list):\n",
    "        print(str(i)+f\" )Calling API for {len(chunk)} albums\")\n",
    "        print(chunk)\n",
    "        albums = sp.albums(chunk) # Get information about the albums from the API\n",
    "        for album in albums['albums']:\n",
    "            copyrights = album.get('copyrights', []) # Get information about the copyrights\n",
    "            copyrights_info = ', '.join([c['text'] for c in copyrights]) if copyrights else \"No copyrights information\"\n",
    "            Album_Data.append({ # Add album information to the list\n",
    "                'Artist':album['artists'][0]['name'],\n",
    "                'Artist_ID':album['artists'][0]['id'],\n",
    "                'Album_ID':album['id'],\n",
    "                'Name':album['name'],\n",
    "                'Type':album['album_type'],\n",
    "                'Genres': ','.join(album.get('genres', [])),\n",
    "                'Label':album.get('label','Unknown'),\n",
    "                'Popularity':album.get('popularity',None),\n",
    "                'Available_Markets':','.join(album.get('available_markets',[])),\n",
    "                'Release_Date':album.get('release_date','Unknow'),\n",
    "                'ReleaseDatePrecision':album.get('release_date_precision','Unknow'),\n",
    "                'TotalTracks':album.get('total_tracks',None),\n",
    "                'Copyrights': copyrights_info,\n",
    "                'Restrictions': album.get('restrictions', {}).get('reason', None),\n",
    "                'External_URL': album.get('external_urls', {}).get('spotify',None),\n",
    "                'Href': album.get('href',None),\n",
    "                'Image': album['images'][0]['url'] if album.get('images') and len(album['images']) > 0 else None,\n",
    "                'Uri': album.get('uri',None)\n",
    "            })\n",
    "            for track in album['tracks']['items']:\n",
    "                Track_Data.append({\n",
    "                    'Artists': ', '.join(artist['name'] for artist in track['artists']),  # Join the artists' names into a string\n",
    "                    'Album_Name':album['name'],\n",
    "                    'Album_ID':album['id'],\n",
    "                    'Track_ID': track['id'],\n",
    "                    'Name': track['name'],\n",
    "                    'Track_Number': track['track_number'],\n",
    "                    'Type': track['type'],\n",
    "                    'AvailableMarkets': ','.join(track.get('available_markets', [])),\n",
    "                    'Disc_Number': track['disc_number'],\n",
    "                    'Duration_ms': track['duration_ms'],\n",
    "                    'Explicit': track['explicit'],\n",
    "                    'External_urls': track['external_urls'].get('spotify') if track.get('external_urls') else None,  # check externalURL\n",
    "                    'Href': track['href'],\n",
    "                    'Restrictions': track.get('restrictions', {}).get('reason', None),\n",
    "                    'Preview_url': track.get('preview_url',None),\n",
    "                    'Uri': track['uri'],\n",
    "                    'Is_Local': track['is_local']\n",
    "                })\n",
    "        i+=1\n",
    "    Album_Data, Track_Data = pd.DataFrame(Album_Data), pd.DataFrame(Track_Data)\n",
    "    Album_Data['Execution_date'] = Execution_date\n",
    "    Track_Data['Execution_date'] = Execution_date\n",
    "    \n",
    "    return Album_Data, Track_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't read cache at: .cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting connect mongoDB...\n",
      "Client connected successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't write token to cache at: .cache\n",
      "Couldn't read cache at: .cache\n",
      "Couldn't write token to cache at: .cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 )Calling API for 7 albums\n",
      "['3dLXfyaG1kYeSQknLs2LP1', '73rKiFhHZatrwJL0B1F6hY', '6bCs4XCCkm9cTwlswlu0VD', '6HpMdN52TfJAwVbmkrFeBN', '2yUhcn7kF408KjNVuMwV2P', '3K4CaKaEcLuJkJZ3lATzrq', '16xW2AvG6yVXJJ0ZYJ5Dlb']\n",
      "The connection to MongoDB has stopped!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Artist_ID</th>\n",
       "      <th>Album_ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Genres</th>\n",
       "      <th>Label</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Available_Markets</th>\n",
       "      <th>Release_Date</th>\n",
       "      <th>ReleaseDatePrecision</th>\n",
       "      <th>TotalTracks</th>\n",
       "      <th>Copyrights</th>\n",
       "      <th>Restrictions</th>\n",
       "      <th>External_URL</th>\n",
       "      <th>Href</th>\n",
       "      <th>Image</th>\n",
       "      <th>Uri</th>\n",
       "      <th>Execution_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wizkid</td>\n",
       "      <td>3tVQdUvClmAT7URs9V3rsp</td>\n",
       "      <td>3dLXfyaG1kYeSQknLs2LP1</td>\n",
       "      <td>Morayo</td>\n",
       "      <td>album</td>\n",
       "      <td></td>\n",
       "      <td>Starboy/RCA Records</td>\n",
       "      <td>0</td>\n",
       "      <td>AD,AE,AL,AM,AO,AT,AU,AZ,BA,BD,BE,BF,BG,BH,BI,B...</td>\n",
       "      <td>2024-11-22</td>\n",
       "      <td>day</td>\n",
       "      <td>16</td>\n",
       "      <td>(P) 2024 RCA Records, under exclusive license ...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://open.spotify.com/album/3dLXfyaG1kYeSQk...</td>\n",
       "      <td>https://api.spotify.com/v1/albums/3dLXfyaG1kYe...</td>\n",
       "      <td>https://i.scdn.co/image/ab67616d0000b273d1947f...</td>\n",
       "      <td>spotify:album:3dLXfyaG1kYeSQknLs2LP1</td>\n",
       "      <td>2024-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wizkid</td>\n",
       "      <td>3tVQdUvClmAT7URs9V3rsp</td>\n",
       "      <td>73rKiFhHZatrwJL0B1F6hY</td>\n",
       "      <td>More Love, Less Ego</td>\n",
       "      <td>album</td>\n",
       "      <td></td>\n",
       "      <td>Starboy/RCA Records</td>\n",
       "      <td>67</td>\n",
       "      <td>AR,AU,AT,BE,BO,BR,BG,CA,CL,CO,CR,CY,CZ,DK,DO,D...</td>\n",
       "      <td>2022-11-11</td>\n",
       "      <td>day</td>\n",
       "      <td>13</td>\n",
       "      <td>(P) 2022 Starboy Entertainment Ltd., under exc...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://open.spotify.com/album/73rKiFhHZatrwJL...</td>\n",
       "      <td>https://api.spotify.com/v1/albums/73rKiFhHZatr...</td>\n",
       "      <td>https://i.scdn.co/image/ab67616d0000b273e944c5...</td>\n",
       "      <td>spotify:album:73rKiFhHZatrwJL0B1F6hY</td>\n",
       "      <td>2024-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wizkid</td>\n",
       "      <td>3tVQdUvClmAT7URs9V3rsp</td>\n",
       "      <td>6bCs4XCCkm9cTwlswlu0VD</td>\n",
       "      <td>Made In Lagos: Deluxe Edition</td>\n",
       "      <td>album</td>\n",
       "      <td></td>\n",
       "      <td>Starboy/RCA Records</td>\n",
       "      <td>64</td>\n",
       "      <td>AR,AU,AT,BE,BO,BR,BG,CA,CL,CO,CR,CY,CZ,DK,DO,D...</td>\n",
       "      <td>2021-08-27</td>\n",
       "      <td>day</td>\n",
       "      <td>18</td>\n",
       "      <td>(P) 2021 Starboy Entertainment Ltd., under exc...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://open.spotify.com/album/6bCs4XCCkm9cTwl...</td>\n",
       "      <td>https://api.spotify.com/v1/albums/6bCs4XCCkm9c...</td>\n",
       "      <td>https://i.scdn.co/image/ab67616d0000b27379ab73...</td>\n",
       "      <td>spotify:album:6bCs4XCCkm9cTwlswlu0VD</td>\n",
       "      <td>2024-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wizkid</td>\n",
       "      <td>3tVQdUvClmAT7URs9V3rsp</td>\n",
       "      <td>6HpMdN52TfJAwVbmkrFeBN</td>\n",
       "      <td>Made In Lagos</td>\n",
       "      <td>album</td>\n",
       "      <td></td>\n",
       "      <td>Starboy/RCA Records</td>\n",
       "      <td>72</td>\n",
       "      <td>AR,AU,AT,BE,BO,BR,BG,CA,CL,CO,CR,CY,CZ,DK,DO,D...</td>\n",
       "      <td>2020-10-29</td>\n",
       "      <td>day</td>\n",
       "      <td>14</td>\n",
       "      <td>(P) 2020 Starboy Entertainment Ltd., under exc...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://open.spotify.com/album/6HpMdN52TfJAwVb...</td>\n",
       "      <td>https://api.spotify.com/v1/albums/6HpMdN52TfJA...</td>\n",
       "      <td>https://i.scdn.co/image/ab67616d0000b27390e89e...</td>\n",
       "      <td>spotify:album:6HpMdN52TfJAwVbmkrFeBN</td>\n",
       "      <td>2024-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wizkid</td>\n",
       "      <td>3tVQdUvClmAT7URs9V3rsp</td>\n",
       "      <td>2yUhcn7kF408KjNVuMwV2P</td>\n",
       "      <td>Sounds From The Other Side</td>\n",
       "      <td>album</td>\n",
       "      <td></td>\n",
       "      <td>Starboy/RCA Records</td>\n",
       "      <td>59</td>\n",
       "      <td>AR,AU,AT,BE,BO,BR,BG,CA,CL,CO,CR,CY,CZ,DK,DO,D...</td>\n",
       "      <td>2017-07-14</td>\n",
       "      <td>day</td>\n",
       "      <td>12</td>\n",
       "      <td>(P) 2017 Starboy Entertainment Ltd., under exc...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://open.spotify.com/album/2yUhcn7kF408KjN...</td>\n",
       "      <td>https://api.spotify.com/v1/albums/2yUhcn7kF408...</td>\n",
       "      <td>https://i.scdn.co/image/ab67616d0000b27313c40d...</td>\n",
       "      <td>spotify:album:2yUhcn7kF408KjNVuMwV2P</td>\n",
       "      <td>2024-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wizkid</td>\n",
       "      <td>3tVQdUvClmAT7URs9V3rsp</td>\n",
       "      <td>3K4CaKaEcLuJkJZ3lATzrq</td>\n",
       "      <td>Ayo</td>\n",
       "      <td>album</td>\n",
       "      <td></td>\n",
       "      <td>Empire Mates Entertainment Ltd.</td>\n",
       "      <td>61</td>\n",
       "      <td>AR,AU,AT,BE,BO,BR,BG,CA,CL,CO,CR,CY,CZ,DK,DO,D...</td>\n",
       "      <td>2014-09-17</td>\n",
       "      <td>day</td>\n",
       "      <td>19</td>\n",
       "      <td>2014 Empire Mates Entertainment Ltd., 2014 Emp...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://open.spotify.com/album/3K4CaKaEcLuJkJZ...</td>\n",
       "      <td>https://api.spotify.com/v1/albums/3K4CaKaEcLuJ...</td>\n",
       "      <td>https://i.scdn.co/image/ab67616d0000b273a61f77...</td>\n",
       "      <td>spotify:album:3K4CaKaEcLuJkJZ3lATzrq</td>\n",
       "      <td>2024-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wizkid</td>\n",
       "      <td>3tVQdUvClmAT7URs9V3rsp</td>\n",
       "      <td>16xW2AvG6yVXJJ0ZYJ5Dlb</td>\n",
       "      <td>Superstar</td>\n",
       "      <td>album</td>\n",
       "      <td></td>\n",
       "      <td>Empire Mates Entertainment Ltd.</td>\n",
       "      <td>57</td>\n",
       "      <td>AR,AU,AT,BE,BO,BR,BG,CA,CL,CO,CR,CY,CZ,DK,DO,D...</td>\n",
       "      <td>2011-06-12</td>\n",
       "      <td>day</td>\n",
       "      <td>17</td>\n",
       "      <td>2022 Empire Mates Entertainment Ltd., 2022 Emp...</td>\n",
       "      <td>None</td>\n",
       "      <td>https://open.spotify.com/album/16xW2AvG6yVXJJ0...</td>\n",
       "      <td>https://api.spotify.com/v1/albums/16xW2AvG6yVX...</td>\n",
       "      <td>https://i.scdn.co/image/ab67616d0000b273475dd4...</td>\n",
       "      <td>spotify:album:16xW2AvG6yVXJJ0ZYJ5Dlb</td>\n",
       "      <td>2024-11-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Artist               Artist_ID                Album_ID  \\\n",
       "0  Wizkid  3tVQdUvClmAT7URs9V3rsp  3dLXfyaG1kYeSQknLs2LP1   \n",
       "1  Wizkid  3tVQdUvClmAT7URs9V3rsp  73rKiFhHZatrwJL0B1F6hY   \n",
       "2  Wizkid  3tVQdUvClmAT7URs9V3rsp  6bCs4XCCkm9cTwlswlu0VD   \n",
       "3  Wizkid  3tVQdUvClmAT7URs9V3rsp  6HpMdN52TfJAwVbmkrFeBN   \n",
       "4  Wizkid  3tVQdUvClmAT7URs9V3rsp  2yUhcn7kF408KjNVuMwV2P   \n",
       "5  Wizkid  3tVQdUvClmAT7URs9V3rsp  3K4CaKaEcLuJkJZ3lATzrq   \n",
       "6  Wizkid  3tVQdUvClmAT7URs9V3rsp  16xW2AvG6yVXJJ0ZYJ5Dlb   \n",
       "\n",
       "                            Name   Type Genres  \\\n",
       "0                         Morayo  album          \n",
       "1            More Love, Less Ego  album          \n",
       "2  Made In Lagos: Deluxe Edition  album          \n",
       "3                  Made In Lagos  album          \n",
       "4     Sounds From The Other Side  album          \n",
       "5                            Ayo  album          \n",
       "6                      Superstar  album          \n",
       "\n",
       "                             Label  Popularity  \\\n",
       "0              Starboy/RCA Records           0   \n",
       "1              Starboy/RCA Records          67   \n",
       "2              Starboy/RCA Records          64   \n",
       "3              Starboy/RCA Records          72   \n",
       "4              Starboy/RCA Records          59   \n",
       "5  Empire Mates Entertainment Ltd.          61   \n",
       "6  Empire Mates Entertainment Ltd.          57   \n",
       "\n",
       "                                   Available_Markets Release_Date  \\\n",
       "0  AD,AE,AL,AM,AO,AT,AU,AZ,BA,BD,BE,BF,BG,BH,BI,B...   2024-11-22   \n",
       "1  AR,AU,AT,BE,BO,BR,BG,CA,CL,CO,CR,CY,CZ,DK,DO,D...   2022-11-11   \n",
       "2  AR,AU,AT,BE,BO,BR,BG,CA,CL,CO,CR,CY,CZ,DK,DO,D...   2021-08-27   \n",
       "3  AR,AU,AT,BE,BO,BR,BG,CA,CL,CO,CR,CY,CZ,DK,DO,D...   2020-10-29   \n",
       "4  AR,AU,AT,BE,BO,BR,BG,CA,CL,CO,CR,CY,CZ,DK,DO,D...   2017-07-14   \n",
       "5  AR,AU,AT,BE,BO,BR,BG,CA,CL,CO,CR,CY,CZ,DK,DO,D...   2014-09-17   \n",
       "6  AR,AU,AT,BE,BO,BR,BG,CA,CL,CO,CR,CY,CZ,DK,DO,D...   2011-06-12   \n",
       "\n",
       "  ReleaseDatePrecision  TotalTracks  \\\n",
       "0                  day           16   \n",
       "1                  day           13   \n",
       "2                  day           18   \n",
       "3                  day           14   \n",
       "4                  day           12   \n",
       "5                  day           19   \n",
       "6                  day           17   \n",
       "\n",
       "                                          Copyrights Restrictions  \\\n",
       "0  (P) 2024 RCA Records, under exclusive license ...         None   \n",
       "1  (P) 2022 Starboy Entertainment Ltd., under exc...         None   \n",
       "2  (P) 2021 Starboy Entertainment Ltd., under exc...         None   \n",
       "3  (P) 2020 Starboy Entertainment Ltd., under exc...         None   \n",
       "4  (P) 2017 Starboy Entertainment Ltd., under exc...         None   \n",
       "5  2014 Empire Mates Entertainment Ltd., 2014 Emp...         None   \n",
       "6  2022 Empire Mates Entertainment Ltd., 2022 Emp...         None   \n",
       "\n",
       "                                        External_URL  \\\n",
       "0  https://open.spotify.com/album/3dLXfyaG1kYeSQk...   \n",
       "1  https://open.spotify.com/album/73rKiFhHZatrwJL...   \n",
       "2  https://open.spotify.com/album/6bCs4XCCkm9cTwl...   \n",
       "3  https://open.spotify.com/album/6HpMdN52TfJAwVb...   \n",
       "4  https://open.spotify.com/album/2yUhcn7kF408KjN...   \n",
       "5  https://open.spotify.com/album/3K4CaKaEcLuJkJZ...   \n",
       "6  https://open.spotify.com/album/16xW2AvG6yVXJJ0...   \n",
       "\n",
       "                                                Href  \\\n",
       "0  https://api.spotify.com/v1/albums/3dLXfyaG1kYe...   \n",
       "1  https://api.spotify.com/v1/albums/73rKiFhHZatr...   \n",
       "2  https://api.spotify.com/v1/albums/6bCs4XCCkm9c...   \n",
       "3  https://api.spotify.com/v1/albums/6HpMdN52TfJA...   \n",
       "4  https://api.spotify.com/v1/albums/2yUhcn7kF408...   \n",
       "5  https://api.spotify.com/v1/albums/3K4CaKaEcLuJ...   \n",
       "6  https://api.spotify.com/v1/albums/16xW2AvG6yVX...   \n",
       "\n",
       "                                               Image  \\\n",
       "0  https://i.scdn.co/image/ab67616d0000b273d1947f...   \n",
       "1  https://i.scdn.co/image/ab67616d0000b273e944c5...   \n",
       "2  https://i.scdn.co/image/ab67616d0000b27379ab73...   \n",
       "3  https://i.scdn.co/image/ab67616d0000b27390e89e...   \n",
       "4  https://i.scdn.co/image/ab67616d0000b27313c40d...   \n",
       "5  https://i.scdn.co/image/ab67616d0000b273a61f77...   \n",
       "6  https://i.scdn.co/image/ab67616d0000b273475dd4...   \n",
       "\n",
       "                                    Uri Execution_date  \n",
       "0  spotify:album:3dLXfyaG1kYeSQknLs2LP1     2024-11-23  \n",
       "1  spotify:album:73rKiFhHZatrwJL0B1F6hY     2024-11-23  \n",
       "2  spotify:album:6bCs4XCCkm9cTwlswlu0VD     2024-11-23  \n",
       "3  spotify:album:6HpMdN52TfJAwVbmkrFeBN     2024-11-23  \n",
       "4  spotify:album:2yUhcn7kF408KjNVuMwV2P     2024-11-23  \n",
       "5  spotify:album:3K4CaKaEcLuJkJZ3lATzrq     2024-11-23  \n",
       "6  spotify:album:16xW2AvG6yVXJJ0ZYJ5Dlb     2024-11-23  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with mongoDB_client(username = 'huynhthuan', password = 'password') as client:\n",
    "    client_operations = mongoDB_operations(client)\n",
    "    daily_artist_data = client_operations.read_data(database_name = 'music_database', collection_name = 'artist_collection', query = {'Execution_date': \"2024-11-23\"})\n",
    "    daily_album_data, daily_track_data = getAlbumData(daily_artist_data, \"2024-11-23\")\n",
    "    \n",
    "    #client_operations.insert_data(database_name = 'music_database', collection_name = 'album_collection', data = daily_album_data, )\n",
    "daily_album_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from spotipy.exceptions import SpotifyException\n",
    "import pandas as pd\n",
    "\n",
    "# Function to divide track id list into small groups (chunks)\n",
    "def chunk_track_ids(track_ids,chunk_size=100):\n",
    "    for i in range(0,len(track_ids),chunk_size):\n",
    "        yield track_ids[i:i+chunk_size]\n",
    "\n",
    "\n",
    "def crawl_track_feature(dfTrack: pd.DataFrame, Execution_date: str): \n",
    "\n",
    "    sp =spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id='5036da07445c484eafb112ed83c0f03a'\n",
    "                                                          ,client_secret='3da98e03edf442148da0f481b2cbc007'))\n",
    "    \n",
    "    Track_Feature_Data =[]\n",
    "    i=1\n",
    "\n",
    "    track_ids = dfTrack['Track_ID'].tolist()\n",
    "    # Split track id list into chunks of 100\n",
    "    for chunk in chunk_track_ids(track_ids):\n",
    "        print(str(i)+f\" )Calling API for {len(chunk)} tracks\")\n",
    "        tracks = sp.audio_features(chunk) # Get information about multiple tracks\n",
    "        if tracks:\n",
    "            for track,track_id in zip(tracks or [],chunk):\n",
    "                if track:\n",
    "                    Track_Feature_Data.append({\n",
    "                        'Track_ID':track_id,\n",
    "                        'Danceability':track.get('danceability',None),\n",
    "                        'Energy':track.get('energy',None),\n",
    "                        'Key':track.get('key',None),\n",
    "                        'Loudness':track.get('loudness',None),\n",
    "                        'Mode':track.get('mode',None),\n",
    "                        'Speechiness':track.get('speechiness',None),\n",
    "                        'Acousticness':track.get('acousticness',None),\n",
    "                        'Instrumentalness':track.get('instrumentalness',None),\n",
    "                        'Liveness':track.get('liveness',None),\n",
    "                        'Valence':track.get('valence',None),\n",
    "                        'Tempo':track.get('tempo',None),\n",
    "                        'Time_signature':track.get('time_signature',None),\n",
    "                        'Track_href':track.get('track_href',None),\n",
    "                        'Type_Feature':track.get('type',None),\n",
    "                        'Analysis_Url':track.get('analysis_url',None)\n",
    "                    })\n",
    "                else:\n",
    "                    Track_Feature_Data.append({\n",
    "                        'Track_ID':track_id,\n",
    "                        'Danceability':None,\n",
    "                        'Energy':None,\n",
    "                        'Key':None,\n",
    "                        'Loudness':None,\n",
    "                        'Mode':None,\n",
    "                        'Speechiness':None,\n",
    "                        'Acousticness':None,\n",
    "                        'Instrumentalness':None,\n",
    "                        'Liveness':None,\n",
    "                        'Valence':None,\n",
    "                        'Tempo':None,\n",
    "                        'Time_signature':None,\n",
    "                        'Track_href':None,\n",
    "                        'Type_Feature':None,\n",
    "                        'Analysis_Url':None\n",
    "                })\n",
    "        i+=1\n",
    "    Track_Feature_Data = pd.DataFrame(Track_Feature_Data)\n",
    "    Track_Feature_Data['Execution_date'] = Execution_date\n",
    "    return Track_Feature_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting connect mongoDB...\n",
      "Client connected successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't read cache at: .cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 )Calling API for 100 tracks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't write token to cache at: .cache\n",
      "Couldn't read cache at: .cache\n",
      "Couldn't write token to cache at: .cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 )Calling API for 9 tracks\n",
      "The connection to MongoDB has stopped!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Track_ID</th>\n",
       "      <th>Danceability</th>\n",
       "      <th>Energy</th>\n",
       "      <th>Key</th>\n",
       "      <th>Loudness</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Speechiness</th>\n",
       "      <th>Acousticness</th>\n",
       "      <th>Instrumentalness</th>\n",
       "      <th>Liveness</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Tempo</th>\n",
       "      <th>Time_signature</th>\n",
       "      <th>Track_href</th>\n",
       "      <th>Type_Feature</th>\n",
       "      <th>Analysis_Url</th>\n",
       "      <th>Execution_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6yIEi8mvrYSi2IgPhQ5Ym9</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.517</td>\n",
       "      <td>11</td>\n",
       "      <td>-9.786</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>0.4480</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.0889</td>\n",
       "      <td>0.752</td>\n",
       "      <td>179.996</td>\n",
       "      <td>4</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/6yIEi8mvrYSi...</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/6yIE...</td>\n",
       "      <td>2024-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4H4l7wkB7UcEMFvfIbGsuq</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.726</td>\n",
       "      <td>5</td>\n",
       "      <td>-5.270</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.0325</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.3450</td>\n",
       "      <td>0.824</td>\n",
       "      <td>113.048</td>\n",
       "      <td>4</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/4H4l7wkB7UcE...</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/4H4l...</td>\n",
       "      <td>2024-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6JvWMLscnWYkgX1zPufIGT</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.874</td>\n",
       "      <td>5</td>\n",
       "      <td>-5.635</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0503</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.393000</td>\n",
       "      <td>0.4110</td>\n",
       "      <td>0.915</td>\n",
       "      <td>118.082</td>\n",
       "      <td>4</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/6JvWMLscnWYk...</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/6JvW...</td>\n",
       "      <td>2024-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2j8n2hpZlEMvtrjJ7n0ZIy</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.622</td>\n",
       "      <td>11</td>\n",
       "      <td>-5.452</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0961</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.644</td>\n",
       "      <td>111.998</td>\n",
       "      <td>4</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/2j8n2hpZlEMv...</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/2j8n...</td>\n",
       "      <td>2024-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0d1rbxaODhWDBLJ7ywooEj</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.569</td>\n",
       "      <td>4</td>\n",
       "      <td>-6.064</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0903</td>\n",
       "      <td>0.3310</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>0.771</td>\n",
       "      <td>95.033</td>\n",
       "      <td>4</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/0d1rbxaODhWD...</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/0d1r...</td>\n",
       "      <td>2024-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>4lrRwgf9ybhqoybJky6i4u</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.879</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.576</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0922</td>\n",
       "      <td>0.2720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2470</td>\n",
       "      <td>0.535</td>\n",
       "      <td>79.890</td>\n",
       "      <td>3</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/4lrRwgf9ybhq...</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/4lrR...</td>\n",
       "      <td>2024-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>4gn99f9Yv3FpMd9UuElz6g</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.866</td>\n",
       "      <td>10</td>\n",
       "      <td>-4.991</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3540</td>\n",
       "      <td>0.2120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.829</td>\n",
       "      <td>179.970</td>\n",
       "      <td>4</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/4gn99f9Yv3Fp...</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/4gn9...</td>\n",
       "      <td>2024-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1Mi7wR12aYRvgsyXYdqRKw</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.738</td>\n",
       "      <td>11</td>\n",
       "      <td>-4.762</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0540</td>\n",
       "      <td>0.4300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0727</td>\n",
       "      <td>0.575</td>\n",
       "      <td>133.264</td>\n",
       "      <td>5</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/1Mi7wR12aYRv...</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/1Mi7...</td>\n",
       "      <td>2024-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>77sMr89rn34HHeNeasZTdP</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.933</td>\n",
       "      <td>5</td>\n",
       "      <td>-5.680</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0831</td>\n",
       "      <td>0.5030</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.0707</td>\n",
       "      <td>0.975</td>\n",
       "      <td>119.000</td>\n",
       "      <td>4</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/77sMr89rn34H...</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/77sM...</td>\n",
       "      <td>2024-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2oWU6r6IvEIvr8GL1hawnp</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.898</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0756</td>\n",
       "      <td>0.1490</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.788</td>\n",
       "      <td>115.000</td>\n",
       "      <td>4</td>\n",
       "      <td>https://api.spotify.com/v1/tracks/2oWU6r6IvEIv...</td>\n",
       "      <td>audio_features</td>\n",
       "      <td>https://api.spotify.com/v1/audio-analysis/2oWU...</td>\n",
       "      <td>2024-11-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows  17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Track_ID  Danceability  Energy  Key  Loudness  Mode  \\\n",
       "0    6yIEi8mvrYSi2IgPhQ5Ym9         0.489   0.517   11    -9.786     0   \n",
       "1    4H4l7wkB7UcEMFvfIbGsuq         0.878   0.726    5    -5.270     1   \n",
       "2    6JvWMLscnWYkgX1zPufIGT         0.775   0.874    5    -5.635     0   \n",
       "3    2j8n2hpZlEMvtrjJ7n0ZIy         0.878   0.622   11    -5.452     0   \n",
       "4    0d1rbxaODhWDBLJ7ywooEj         0.840   0.569    4    -6.064     0   \n",
       "..                      ...           ...     ...  ...       ...   ...   \n",
       "104  4lrRwgf9ybhqoybJky6i4u         0.615   0.879    2    -2.576     0   \n",
       "105  4gn99f9Yv3FpMd9UuElz6g         0.470   0.866   10    -4.991     1   \n",
       "106  1Mi7wR12aYRvgsyXYdqRKw         0.701   0.738   11    -4.762     0   \n",
       "107  77sMr89rn34HHeNeasZTdP         0.784   0.933    5    -5.680     1   \n",
       "108  2oWU6r6IvEIvr8GL1hawnp         0.696   0.899    0    -6.898     0   \n",
       "\n",
       "     Speechiness  Acousticness  Instrumentalness  Liveness  Valence    Tempo  \\\n",
       "0         0.1480        0.4480          0.000052    0.0889    0.752  179.996   \n",
       "1         0.1650        0.0325          0.000107    0.3450    0.824  113.048   \n",
       "2         0.0503        0.1430          0.393000    0.4110    0.915  118.082   \n",
       "3         0.0961        0.0369          0.001070    0.1100    0.644  111.998   \n",
       "4         0.0903        0.3310          0.000858    0.1150    0.771   95.033   \n",
       "..           ...           ...               ...       ...      ...      ...   \n",
       "104       0.0922        0.2720          0.000000    0.2470    0.535   79.890   \n",
       "105       0.3540        0.2120          0.000000    0.1050    0.829  179.970   \n",
       "106       0.0540        0.4300          0.000000    0.0727    0.575  133.264   \n",
       "107       0.0831        0.5030          0.000072    0.0707    0.975  119.000   \n",
       "108       0.0756        0.1490          0.000001    0.1010    0.788  115.000   \n",
       "\n",
       "     Time_signature                                         Track_href  \\\n",
       "0                 4  https://api.spotify.com/v1/tracks/6yIEi8mvrYSi...   \n",
       "1                 4  https://api.spotify.com/v1/tracks/4H4l7wkB7UcE...   \n",
       "2                 4  https://api.spotify.com/v1/tracks/6JvWMLscnWYk...   \n",
       "3                 4  https://api.spotify.com/v1/tracks/2j8n2hpZlEMv...   \n",
       "4                 4  https://api.spotify.com/v1/tracks/0d1rbxaODhWD...   \n",
       "..              ...                                                ...   \n",
       "104               3  https://api.spotify.com/v1/tracks/4lrRwgf9ybhq...   \n",
       "105               4  https://api.spotify.com/v1/tracks/4gn99f9Yv3Fp...   \n",
       "106               5  https://api.spotify.com/v1/tracks/1Mi7wR12aYRv...   \n",
       "107               4  https://api.spotify.com/v1/tracks/77sMr89rn34H...   \n",
       "108               4  https://api.spotify.com/v1/tracks/2oWU6r6IvEIv...   \n",
       "\n",
       "       Type_Feature                                       Analysis_Url  \\\n",
       "0    audio_features  https://api.spotify.com/v1/audio-analysis/6yIE...   \n",
       "1    audio_features  https://api.spotify.com/v1/audio-analysis/4H4l...   \n",
       "2    audio_features  https://api.spotify.com/v1/audio-analysis/6JvW...   \n",
       "3    audio_features  https://api.spotify.com/v1/audio-analysis/2j8n...   \n",
       "4    audio_features  https://api.spotify.com/v1/audio-analysis/0d1r...   \n",
       "..              ...                                                ...   \n",
       "104  audio_features  https://api.spotify.com/v1/audio-analysis/4lrR...   \n",
       "105  audio_features  https://api.spotify.com/v1/audio-analysis/4gn9...   \n",
       "106  audio_features  https://api.spotify.com/v1/audio-analysis/1Mi7...   \n",
       "107  audio_features  https://api.spotify.com/v1/audio-analysis/77sM...   \n",
       "108  audio_features  https://api.spotify.com/v1/audio-analysis/2oWU...   \n",
       "\n",
       "    Execution_date  \n",
       "0       2024-11-23  \n",
       "1       2024-11-23  \n",
       "2       2024-11-23  \n",
       "3       2024-11-23  \n",
       "4       2024-11-23  \n",
       "..             ...  \n",
       "104     2024-11-23  \n",
       "105     2024-11-23  \n",
       "106     2024-11-23  \n",
       "107     2024-11-23  \n",
       "108     2024-11-23  \n",
       "\n",
       "[109 rows x 17 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with mongoDB_client(username = 'huynhthuan', password = 'password') as client:\n",
    "    client_operations = mongoDB_operations(client)\n",
    "    daily_track_data = client_operations.read_data(database_name = 'music_database', collection_name = 'track_collection', query = {'Execution_date': \"2024-11-23\"})\n",
    "    daily_track_feature_data = crawl_track_feature(daily_track_data, \"2024-11-23\")\n",
    "\n",
    "daily_track_feature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting connect mongoDB...\n",
      "Client connected successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't read cache at: .cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2ytgLVGoS1CoHnr8qBkmii', '2ZxRj0Pb0rsf9QCxZPRKD6', '6VwdDEFy5uMMtQ74yDjKS6', '3ZSArIFQxs9ZqPdhJzftnZ', '3BQfIA33wxsyvdEZobZYDI', '3uqGEgvpnuF1dpZ7j6pzcJ', '2SEhW9lX9AFNYW3KRuzd9o', '14LxTJPpVz6VsfSU8dpdXu', '6xG3Lic1DsY3SdwS74P48i', '5xsDsGcw7wNcZSXAsPlpD5', '6yIEi8mvrYSi2IgPhQ5Ym9', '4H4l7wkB7UcEMFvfIbGsuq', '6JvWMLscnWYkgX1zPufIGT', '2j8n2hpZlEMvtrjJ7n0ZIy', '0d1rbxaODhWDBLJ7ywooEj', '7Ijsqi4y74qZB04PaxnYBq', '0EI0H7VQ7SNk7cwpuxG1DC', '2GZNKi3rgaT1ZKpkKDkA0Y', '2hZttcpkiVBA6thTLFDaS9', '7JKGyMt8qkTaUHXFpxgjY2', '3Ae97zTfGTtJzDfmCweNn8', '4hH9eVrAzlKR0jbLmHUkPD', '1t6yJs1WSeetp1OdFf9oHJ', '3fGdbjANHhuAhNo0f4POgw', '4sHR4SfCBR7iaoi1PNi1W1', '5HeW3DSp2lsfcXXRuJkZUy', '213Ymlnss7DUsqG7Ut1HW0', '0sHyVWNc3nfxAYOTOwOlPx', '2HYcI74UpbMMxLZ1m7Xnfn', '1DA2ADZs6O28y2rmdmpekw', '5xFQlD4ITOWjZdHXjNghbX', '1w5cQpiBrPmwcIgRrz1Jcc', '5iTsvRTl65FFgqZZLNLEOC', '2arzk4Lrn05DmGTPvm9JSx', '0Z5HVNSXGXHf7MDb0zJHVW', '7BlmNYOn2IRWTdlk2qypvM', '0TFZDcp2Axfuztj9fr7oeC', '6MtFM9kbhhuN04rwDxPvn4', '1xQQJELCro7Fys7pUnHVYv', '2cuZU2ssATNb1Ey1SB1V1X', '5TOoNqmK73ev73C5JiJ5yC', '1i2HKpJ5A9ebyPxrbgoi3B', '0N4ufAwa0NwpumEFqVXtek', '1eUTZXovJXU3Eb1Gyk1ibK', '430wk0UdTXB5gaOCjHHgfq', '4eg9kaGGZjfQpxLb8isiO4', '08jx6kRJNOggEc1QLFYacW', '3IjsZ7wsE0aQgtb2crMbcC', '4TqssI52mf1vE6K6ZjZq4A', '7JATnENsPMMQWtvMsQnW23', '1i63fAE1LEghiIVEbrTtWr', '0WQb1ms6xMQ0gKrCUqraYk', '0ui4TcK8tcHWZ4JnBR6lIa', '60lUecrFeE2t6QMJ1Nmsve', '6jdTkoEaer7XNGSblczoSu', '6mAdcIFP25eb37HjkzglSh', '1MZtr7IH5qtjIkqrXj8WOJ', '19ehhyzTggrwMzaOzYt85g', '51mLQ3w7yR7vjdSTFLWaY5', '1JiR4RJaZlbZ5b3HG8jkeL', '1gAgb7hnZ9AAJ5MCcvSKqJ', '3QO1m6i0nsrp8aOnapvbkx', '2IwiLJ3VA4cZUWBcu18DAR', '6m4SEnC7eZLrgroEvwAmCF', '5C3vZiMOn2KHMbNQOhL6oQ', '4204hwPYuToiuSunPFUoML', '7BxWEstQxXtjczBE7ErYrE', '5FG7Tl93LdH117jEKYl3Cm', '1F7xMfEowsK6i0kODlO0Xl', '6ocTwwhYsATcgNkKZuw95O', '7CTTKdRhiXHDTWmgjnP68l', '1XuInr77ZWegR201wmDYp1', '2zYs4BonN2ydkbrRk333SN', '0erJdrkQUJlA7nbtn1qxQR', '4FRKGdgOh9G5sub5GiknfS', '2hKLu5akQZ9BbrCQVXM2cZ', '284b45rER2dZCBVCkCQNEk', '0LmD6LnXYecaaMZKG8bW12', '4a2rDgkDZVSN660vkhE4mE', '7KP43bdMn2KDAchHph8rNP', '73J7ROhWCZB9UDHombzlSr', '47AMYhY7w49L3rv6UZdlXV', '43Hf4nALLvqdVAPTzMtxn4', '2vXJl10Kiu3IqMo8moFBFK', '31FgIP47GdYyYKLqrs2qk3', '002qGVBq254LJPG0t5uNv8', '6QcYfnnQN7FfCaPqOjHWVT', '1AtBzcUzKLh4BGwXhFA9K6', '3l4MGkR30Qku3pNyUqQD06', '5dmzKrF5dzyjsLh85JFUZT', '3KdRqhzcAjX1atP3HIqlVv', '6GkOrVjHU9BIWR7PZj6r2V', '77o8apfpn8YmAfYXYtr391', '5Uv7iAjsSToLIm0VpAbCY5', '6AnUHNm5KuY2TFwj2VeWkN', '57DCnDDUscpxL1b0SM8lz8', '1MOG45KKhwGqjE0OXm3IRq', '4QV7pRkY0E5TFGrA0YNssj', '3hTLw115ohhFJWmiusvKGU', '09k9cybGimJMqymIfXkWYA']\n",
      "1 )Calling API for 100 tracks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't write token to cache at: .cache\n",
      "HTTP Error for GET to https://api.spotify.com/v1/audio-features/?ids=2ytgLVGoS1CoHnr8qBkmii,2ZxRj0Pb0rsf9QCxZPRKD6,6VwdDEFy5uMMtQ74yDjKS6,3ZSArIFQxs9ZqPdhJzftnZ,3BQfIA33wxsyvdEZobZYDI,3uqGEgvpnuF1dpZ7j6pzcJ,2SEhW9lX9AFNYW3KRuzd9o,14LxTJPpVz6VsfSU8dpdXu,6xG3Lic1DsY3SdwS74P48i,5xsDsGcw7wNcZSXAsPlpD5,6yIEi8mvrYSi2IgPhQ5Ym9,4H4l7wkB7UcEMFvfIbGsuq,6JvWMLscnWYkgX1zPufIGT,2j8n2hpZlEMvtrjJ7n0ZIy,0d1rbxaODhWDBLJ7ywooEj,7Ijsqi4y74qZB04PaxnYBq,0EI0H7VQ7SNk7cwpuxG1DC,2GZNKi3rgaT1ZKpkKDkA0Y,2hZttcpkiVBA6thTLFDaS9,7JKGyMt8qkTaUHXFpxgjY2,3Ae97zTfGTtJzDfmCweNn8,4hH9eVrAzlKR0jbLmHUkPD,1t6yJs1WSeetp1OdFf9oHJ,3fGdbjANHhuAhNo0f4POgw,4sHR4SfCBR7iaoi1PNi1W1,5HeW3DSp2lsfcXXRuJkZUy,213Ymlnss7DUsqG7Ut1HW0,0sHyVWNc3nfxAYOTOwOlPx,2HYcI74UpbMMxLZ1m7Xnfn,1DA2ADZs6O28y2rmdmpekw,5xFQlD4ITOWjZdHXjNghbX,1w5cQpiBrPmwcIgRrz1Jcc,5iTsvRTl65FFgqZZLNLEOC,2arzk4Lrn05DmGTPvm9JSx,0Z5HVNSXGXHf7MDb0zJHVW,7BlmNYOn2IRWTdlk2qypvM,0TFZDcp2Axfuztj9fr7oeC,6MtFM9kbhhuN04rwDxPvn4,1xQQJELCro7Fys7pUnHVYv,2cuZU2ssATNb1Ey1SB1V1X,5TOoNqmK73ev73C5JiJ5yC,1i2HKpJ5A9ebyPxrbgoi3B,0N4ufAwa0NwpumEFqVXtek,1eUTZXovJXU3Eb1Gyk1ibK,430wk0UdTXB5gaOCjHHgfq,4eg9kaGGZjfQpxLb8isiO4,08jx6kRJNOggEc1QLFYacW,3IjsZ7wsE0aQgtb2crMbcC,4TqssI52mf1vE6K6ZjZq4A,7JATnENsPMMQWtvMsQnW23,1i63fAE1LEghiIVEbrTtWr,0WQb1ms6xMQ0gKrCUqraYk,0ui4TcK8tcHWZ4JnBR6lIa,60lUecrFeE2t6QMJ1Nmsve,6jdTkoEaer7XNGSblczoSu,6mAdcIFP25eb37HjkzglSh,1MZtr7IH5qtjIkqrXj8WOJ,19ehhyzTggrwMzaOzYt85g,51mLQ3w7yR7vjdSTFLWaY5,1JiR4RJaZlbZ5b3HG8jkeL,1gAgb7hnZ9AAJ5MCcvSKqJ,3QO1m6i0nsrp8aOnapvbkx,2IwiLJ3VA4cZUWBcu18DAR,6m4SEnC7eZLrgroEvwAmCF,5C3vZiMOn2KHMbNQOhL6oQ,4204hwPYuToiuSunPFUoML,7BxWEstQxXtjczBE7ErYrE,5FG7Tl93LdH117jEKYl3Cm,1F7xMfEowsK6i0kODlO0Xl,6ocTwwhYsATcgNkKZuw95O,7CTTKdRhiXHDTWmgjnP68l,1XuInr77ZWegR201wmDYp1,2zYs4BonN2ydkbrRk333SN,0erJdrkQUJlA7nbtn1qxQR,4FRKGdgOh9G5sub5GiknfS,2hKLu5akQZ9BbrCQVXM2cZ,284b45rER2dZCBVCkCQNEk,0LmD6LnXYecaaMZKG8bW12,4a2rDgkDZVSN660vkhE4mE,7KP43bdMn2KDAchHph8rNP,73J7ROhWCZB9UDHombzlSr,47AMYhY7w49L3rv6UZdlXV,43Hf4nALLvqdVAPTzMtxn4,2vXJl10Kiu3IqMo8moFBFK,31FgIP47GdYyYKLqrs2qk3,002qGVBq254LJPG0t5uNv8,6QcYfnnQN7FfCaPqOjHWVT,1AtBzcUzKLh4BGwXhFA9K6,3l4MGkR30Qku3pNyUqQD06,5dmzKrF5dzyjsLh85JFUZT,3KdRqhzcAjX1atP3HIqlVv,6GkOrVjHU9BIWR7PZj6r2V,77o8apfpn8YmAfYXYtr391,5Uv7iAjsSToLIm0VpAbCY5,6AnUHNm5KuY2TFwj2VeWkN,57DCnDDUscpxL1b0SM8lz8,1MOG45KKhwGqjE0OXm3IRq,4QV7pRkY0E5TFGrA0YNssj,3hTLw115ohhFJWmiusvKGU,09k9cybGimJMqymIfXkWYA with Params: {} returned 403 due to None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The connection to MongoDB has stopped!\n"
     ]
    },
    {
     "ename": "SpotifyException",
     "evalue": "http status: 403, code:-1 - https://api.spotify.com/v1/audio-features/?ids=2ytgLVGoS1CoHnr8qBkmii,2ZxRj0Pb0rsf9QCxZPRKD6,6VwdDEFy5uMMtQ74yDjKS6,3ZSArIFQxs9ZqPdhJzftnZ,3BQfIA33wxsyvdEZobZYDI,3uqGEgvpnuF1dpZ7j6pzcJ,2SEhW9lX9AFNYW3KRuzd9o,14LxTJPpVz6VsfSU8dpdXu,6xG3Lic1DsY3SdwS74P48i,5xsDsGcw7wNcZSXAsPlpD5,6yIEi8mvrYSi2IgPhQ5Ym9,4H4l7wkB7UcEMFvfIbGsuq,6JvWMLscnWYkgX1zPufIGT,2j8n2hpZlEMvtrjJ7n0ZIy,0d1rbxaODhWDBLJ7ywooEj,7Ijsqi4y74qZB04PaxnYBq,0EI0H7VQ7SNk7cwpuxG1DC,2GZNKi3rgaT1ZKpkKDkA0Y,2hZttcpkiVBA6thTLFDaS9,7JKGyMt8qkTaUHXFpxgjY2,3Ae97zTfGTtJzDfmCweNn8,4hH9eVrAzlKR0jbLmHUkPD,1t6yJs1WSeetp1OdFf9oHJ,3fGdbjANHhuAhNo0f4POgw,4sHR4SfCBR7iaoi1PNi1W1,5HeW3DSp2lsfcXXRuJkZUy,213Ymlnss7DUsqG7Ut1HW0,0sHyVWNc3nfxAYOTOwOlPx,2HYcI74UpbMMxLZ1m7Xnfn,1DA2ADZs6O28y2rmdmpekw,5xFQlD4ITOWjZdHXjNghbX,1w5cQpiBrPmwcIgRrz1Jcc,5iTsvRTl65FFgqZZLNLEOC,2arzk4Lrn05DmGTPvm9JSx,0Z5HVNSXGXHf7MDb0zJHVW,7BlmNYOn2IRWTdlk2qypvM,0TFZDcp2Axfuztj9fr7oeC,6MtFM9kbhhuN04rwDxPvn4,1xQQJELCro7Fys7pUnHVYv,2cuZU2ssATNb1Ey1SB1V1X,5TOoNqmK73ev73C5JiJ5yC,1i2HKpJ5A9ebyPxrbgoi3B,0N4ufAwa0NwpumEFqVXtek,1eUTZXovJXU3Eb1Gyk1ibK,430wk0UdTXB5gaOCjHHgfq,4eg9kaGGZjfQpxLb8isiO4,08jx6kRJNOggEc1QLFYacW,3IjsZ7wsE0aQgtb2crMbcC,4TqssI52mf1vE6K6ZjZq4A,7JATnENsPMMQWtvMsQnW23,1i63fAE1LEghiIVEbrTtWr,0WQb1ms6xMQ0gKrCUqraYk,0ui4TcK8tcHWZ4JnBR6lIa,60lUecrFeE2t6QMJ1Nmsve,6jdTkoEaer7XNGSblczoSu,6mAdcIFP25eb37HjkzglSh,1MZtr7IH5qtjIkqrXj8WOJ,19ehhyzTggrwMzaOzYt85g,51mLQ3w7yR7vjdSTFLWaY5,1JiR4RJaZlbZ5b3HG8jkeL,1gAgb7hnZ9AAJ5MCcvSKqJ,3QO1m6i0nsrp8aOnapvbkx,2IwiLJ3VA4cZUWBcu18DAR,6m4SEnC7eZLrgroEvwAmCF,5C3vZiMOn2KHMbNQOhL6oQ,4204hwPYuToiuSunPFUoML,7BxWEstQxXtjczBE7ErYrE,5FG7Tl93LdH117jEKYl3Cm,1F7xMfEowsK6i0kODlO0Xl,6ocTwwhYsATcgNkKZuw95O,7CTTKdRhiXHDTWmgjnP68l,1XuInr77ZWegR201wmDYp1,2zYs4BonN2ydkbrRk333SN,0erJdrkQUJlA7nbtn1qxQR,4FRKGdgOh9G5sub5GiknfS,2hKLu5akQZ9BbrCQVXM2cZ,284b45rER2dZCBVCkCQNEk,0LmD6LnXYecaaMZKG8bW12,4a2rDgkDZVSN660vkhE4mE,7KP43bdMn2KDAchHph8rNP,73J7ROhWCZB9UDHombzlSr,47AMYhY7w49L3rv6UZdlXV,43Hf4nALLvqdVAPTzMtxn4,2vXJl10Kiu3IqMo8moFBFK,31FgIP47GdYyYKLqrs2qk3,002qGVBq254LJPG0t5uNv8,6QcYfnnQN7FfCaPqOjHWVT,1AtBzcUzKLh4BGwXhFA9K6,3l4MGkR30Qku3pNyUqQD06,5dmzKrF5dzyjsLh85JFUZT,3KdRqhzcAjX1atP3HIqlVv,6GkOrVjHU9BIWR7PZj6r2V,77o8apfpn8YmAfYXYtr391,5Uv7iAjsSToLIm0VpAbCY5,6AnUHNm5KuY2TFwj2VeWkN,57DCnDDUscpxL1b0SM8lz8,1MOG45KKhwGqjE0OXm3IRq,4QV7pRkY0E5TFGrA0YNssj,3hTLw115ohhFJWmiusvKGU,09k9cybGimJMqymIfXkWYA:\n None, reason: None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/spotipy/client.py:275\u001b[0m, in \u001b[0;36mSpotify._internal_call\u001b[0;34m(self, method, url, payload, params)\u001b[0m\n\u001b[1;32m    270\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    271\u001b[0m     method, url, headers\u001b[38;5;241m=\u001b[39mheaders, proxies\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxies,\n\u001b[1;32m    272\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequests_timeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs\n\u001b[1;32m    273\u001b[0m )\n\u001b[0;32m--> 275\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m results \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://api.spotify.com/v1/audio-features/?ids=2ytgLVGoS1CoHnr8qBkmii,2ZxRj0Pb0rsf9QCxZPRKD6,6VwdDEFy5uMMtQ74yDjKS6,3ZSArIFQxs9ZqPdhJzftnZ,3BQfIA33wxsyvdEZobZYDI,3uqGEgvpnuF1dpZ7j6pzcJ,2SEhW9lX9AFNYW3KRuzd9o,14LxTJPpVz6VsfSU8dpdXu,6xG3Lic1DsY3SdwS74P48i,5xsDsGcw7wNcZSXAsPlpD5,6yIEi8mvrYSi2IgPhQ5Ym9,4H4l7wkB7UcEMFvfIbGsuq,6JvWMLscnWYkgX1zPufIGT,2j8n2hpZlEMvtrjJ7n0ZIy,0d1rbxaODhWDBLJ7ywooEj,7Ijsqi4y74qZB04PaxnYBq,0EI0H7VQ7SNk7cwpuxG1DC,2GZNKi3rgaT1ZKpkKDkA0Y,2hZttcpkiVBA6thTLFDaS9,7JKGyMt8qkTaUHXFpxgjY2,3Ae97zTfGTtJzDfmCweNn8,4hH9eVrAzlKR0jbLmHUkPD,1t6yJs1WSeetp1OdFf9oHJ,3fGdbjANHhuAhNo0f4POgw,4sHR4SfCBR7iaoi1PNi1W1,5HeW3DSp2lsfcXXRuJkZUy,213Ymlnss7DUsqG7Ut1HW0,0sHyVWNc3nfxAYOTOwOlPx,2HYcI74UpbMMxLZ1m7Xnfn,1DA2ADZs6O28y2rmdmpekw,5xFQlD4ITOWjZdHXjNghbX,1w5cQpiBrPmwcIgRrz1Jcc,5iTsvRTl65FFgqZZLNLEOC,2arzk4Lrn05DmGTPvm9JSx,0Z5HVNSXGXHf7MDb0zJHVW,7BlmNYOn2IRWTdlk2qypvM,0TFZDcp2Axfuztj9fr7oeC,6MtFM9kbhhuN04rwDxPvn4,1xQQJELCro7Fys7pUnHVYv,2cuZU2ssATNb1Ey1SB1V1X,5TOoNqmK73ev73C5JiJ5yC,1i2HKpJ5A9ebyPxrbgoi3B,0N4ufAwa0NwpumEFqVXtek,1eUTZXovJXU3Eb1Gyk1ibK,430wk0UdTXB5gaOCjHHgfq,4eg9kaGGZjfQpxLb8isiO4,08jx6kRJNOggEc1QLFYacW,3IjsZ7wsE0aQgtb2crMbcC,4TqssI52mf1vE6K6ZjZq4A,7JATnENsPMMQWtvMsQnW23,1i63fAE1LEghiIVEbrTtWr,0WQb1ms6xMQ0gKrCUqraYk,0ui4TcK8tcHWZ4JnBR6lIa,60lUecrFeE2t6QMJ1Nmsve,6jdTkoEaer7XNGSblczoSu,6mAdcIFP25eb37HjkzglSh,1MZtr7IH5qtjIkqrXj8WOJ,19ehhyzTggrwMzaOzYt85g,51mLQ3w7yR7vjdSTFLWaY5,1JiR4RJaZlbZ5b3HG8jkeL,1gAgb7hnZ9AAJ5MCcvSKqJ,3QO1m6i0nsrp8aOnapvbkx,2IwiLJ3VA4cZUWBcu18DAR,6m4SEnC7eZLrgroEvwAmCF,5C3vZiMOn2KHMbNQOhL6oQ,4204hwPYuToiuSunPFUoML,7BxWEstQxXtjczBE7ErYrE,5FG7Tl93LdH117jEKYl3Cm,1F7xMfEowsK6i0kODlO0Xl,6ocTwwhYsATcgNkKZuw95O,7CTTKdRhiXHDTWmgjnP68l,1XuInr77ZWegR201wmDYp1,2zYs4BonN2ydkbrRk333SN,0erJdrkQUJlA7nbtn1qxQR,4FRKGdgOh9G5sub5GiknfS,2hKLu5akQZ9BbrCQVXM2cZ,284b45rER2dZCBVCkCQNEk,0LmD6LnXYecaaMZKG8bW12,4a2rDgkDZVSN660vkhE4mE,7KP43bdMn2KDAchHph8rNP,73J7ROhWCZB9UDHombzlSr,47AMYhY7w49L3rv6UZdlXV,43Hf4nALLvqdVAPTzMtxn4,2vXJl10Kiu3IqMo8moFBFK,31FgIP47GdYyYKLqrs2qk3,002qGVBq254LJPG0t5uNv8,6QcYfnnQN7FfCaPqOjHWVT,1AtBzcUzKLh4BGwXhFA9K6,3l4MGkR30Qku3pNyUqQD06,5dmzKrF5dzyjsLh85JFUZT,3KdRqhzcAjX1atP3HIqlVv,6GkOrVjHU9BIWR7PZj6r2V,77o8apfpn8YmAfYXYtr391,5Uv7iAjsSToLIm0VpAbCY5,6AnUHNm5KuY2TFwj2VeWkN,57DCnDDUscpxL1b0SM8lz8,1MOG45KKhwGqjE0OXm3IRq,4QV7pRkY0E5TFGrA0YNssj,3hTLw115ohhFJWmiusvKGU,09k9cybGimJMqymIfXkWYA",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSpotifyException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 77\u001b[0m\n\u001b[1;32m     74\u001b[0m         daily_track_feature_data \u001b[38;5;241m=\u001b[39m crawl_track_feature(daily_track_data, Execution_date)\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;66;03m#client_operations.insert_data(database_name = 'music_database', collection_name = 'trackfeature_collection', data = daily_track_feature_data)\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m \u001b[43mload_daily_track_feature_mongoDB\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2024-11-28\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 74\u001b[0m, in \u001b[0;36mload_daily_track_feature_mongoDB\u001b[0;34m(Execution_date)\u001b[0m\n\u001b[1;32m     72\u001b[0m client_operations \u001b[38;5;241m=\u001b[39m mongoDB_operations(client)\n\u001b[1;32m     73\u001b[0m daily_track_data \u001b[38;5;241m=\u001b[39m client_operations\u001b[38;5;241m.\u001b[39mread_data(database_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmusic_database\u001b[39m\u001b[38;5;124m'\u001b[39m, collection_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrack_collection\u001b[39m\u001b[38;5;124m'\u001b[39m, query \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExecution_date\u001b[39m\u001b[38;5;124m'\u001b[39m: Execution_date})\n\u001b[0;32m---> 74\u001b[0m daily_track_feature_data \u001b[38;5;241m=\u001b[39m \u001b[43mcrawl_track_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdaily_track_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mExecution_date\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 24\u001b[0m, in \u001b[0;36mcrawl_track_feature\u001b[0;34m(dfTrack, Execution_date)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(chunk)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(i)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m )Calling API for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunk)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tracks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m tracks \u001b[38;5;241m=\u001b[39m \u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Get information about multiple tracks\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracks:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m track,track_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tracks \u001b[38;5;129;01mor\u001b[39;00m [],chunk):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/spotipy/client.py:1753\u001b[0m, in \u001b[0;36mSpotify.audio_features\u001b[0;34m(self, tracks)\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1752\u001b[0m     tlist \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_id(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack\u001b[39m\u001b[38;5;124m\"\u001b[39m, t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tracks]\n\u001b[0;32m-> 1753\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio-features/?ids=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtlist\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;66;03m# the response has changed, look for the new style first, and if\u001b[39;00m\n\u001b[1;32m   1755\u001b[0m \u001b[38;5;66;03m# it's not there, fallback on the old style\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio_features\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/spotipy/client.py:327\u001b[0m, in \u001b[0;36mSpotify._get\u001b[0;34m(self, url, args, payload, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args:\n\u001b[1;32m    325\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(args)\n\u001b[0;32m--> 327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/spotipy/client.py:297\u001b[0m, in \u001b[0;36mSpotify._internal_call\u001b[0;34m(self, method, url, payload, params)\u001b[0m\n\u001b[1;32m    290\u001b[0m         reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHTTP Error for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with Params: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m returned \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m due to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    294\u001b[0m         method, url, args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m), response\u001b[38;5;241m.\u001b[39mstatus_code, msg\n\u001b[1;32m    295\u001b[0m     )\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SpotifyException(\n\u001b[1;32m    298\u001b[0m         response\u001b[38;5;241m.\u001b[39mstatus_code,\n\u001b[1;32m    299\u001b[0m         \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    301\u001b[0m         reason\u001b[38;5;241m=\u001b[39mreason,\n\u001b[1;32m    302\u001b[0m         headers\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    303\u001b[0m     )\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRetryError \u001b[38;5;28;01mas\u001b[39;00m retry_error:\n\u001b[1;32m    305\u001b[0m     request \u001b[38;5;241m=\u001b[39m retry_error\u001b[38;5;241m.\u001b[39mrequest\n",
      "\u001b[0;31mSpotifyException\u001b[0m: http status: 403, code:-1 - https://api.spotify.com/v1/audio-features/?ids=2ytgLVGoS1CoHnr8qBkmii,2ZxRj0Pb0rsf9QCxZPRKD6,6VwdDEFy5uMMtQ74yDjKS6,3ZSArIFQxs9ZqPdhJzftnZ,3BQfIA33wxsyvdEZobZYDI,3uqGEgvpnuF1dpZ7j6pzcJ,2SEhW9lX9AFNYW3KRuzd9o,14LxTJPpVz6VsfSU8dpdXu,6xG3Lic1DsY3SdwS74P48i,5xsDsGcw7wNcZSXAsPlpD5,6yIEi8mvrYSi2IgPhQ5Ym9,4H4l7wkB7UcEMFvfIbGsuq,6JvWMLscnWYkgX1zPufIGT,2j8n2hpZlEMvtrjJ7n0ZIy,0d1rbxaODhWDBLJ7ywooEj,7Ijsqi4y74qZB04PaxnYBq,0EI0H7VQ7SNk7cwpuxG1DC,2GZNKi3rgaT1ZKpkKDkA0Y,2hZttcpkiVBA6thTLFDaS9,7JKGyMt8qkTaUHXFpxgjY2,3Ae97zTfGTtJzDfmCweNn8,4hH9eVrAzlKR0jbLmHUkPD,1t6yJs1WSeetp1OdFf9oHJ,3fGdbjANHhuAhNo0f4POgw,4sHR4SfCBR7iaoi1PNi1W1,5HeW3DSp2lsfcXXRuJkZUy,213Ymlnss7DUsqG7Ut1HW0,0sHyVWNc3nfxAYOTOwOlPx,2HYcI74UpbMMxLZ1m7Xnfn,1DA2ADZs6O28y2rmdmpekw,5xFQlD4ITOWjZdHXjNghbX,1w5cQpiBrPmwcIgRrz1Jcc,5iTsvRTl65FFgqZZLNLEOC,2arzk4Lrn05DmGTPvm9JSx,0Z5HVNSXGXHf7MDb0zJHVW,7BlmNYOn2IRWTdlk2qypvM,0TFZDcp2Axfuztj9fr7oeC,6MtFM9kbhhuN04rwDxPvn4,1xQQJELCro7Fys7pUnHVYv,2cuZU2ssATNb1Ey1SB1V1X,5TOoNqmK73ev73C5JiJ5yC,1i2HKpJ5A9ebyPxrbgoi3B,0N4ufAwa0NwpumEFqVXtek,1eUTZXovJXU3Eb1Gyk1ibK,430wk0UdTXB5gaOCjHHgfq,4eg9kaGGZjfQpxLb8isiO4,08jx6kRJNOggEc1QLFYacW,3IjsZ7wsE0aQgtb2crMbcC,4TqssI52mf1vE6K6ZjZq4A,7JATnENsPMMQWtvMsQnW23,1i63fAE1LEghiIVEbrTtWr,0WQb1ms6xMQ0gKrCUqraYk,0ui4TcK8tcHWZ4JnBR6lIa,60lUecrFeE2t6QMJ1Nmsve,6jdTkoEaer7XNGSblczoSu,6mAdcIFP25eb37HjkzglSh,1MZtr7IH5qtjIkqrXj8WOJ,19ehhyzTggrwMzaOzYt85g,51mLQ3w7yR7vjdSTFLWaY5,1JiR4RJaZlbZ5b3HG8jkeL,1gAgb7hnZ9AAJ5MCcvSKqJ,3QO1m6i0nsrp8aOnapvbkx,2IwiLJ3VA4cZUWBcu18DAR,6m4SEnC7eZLrgroEvwAmCF,5C3vZiMOn2KHMbNQOhL6oQ,4204hwPYuToiuSunPFUoML,7BxWEstQxXtjczBE7ErYrE,5FG7Tl93LdH117jEKYl3Cm,1F7xMfEowsK6i0kODlO0Xl,6ocTwwhYsATcgNkKZuw95O,7CTTKdRhiXHDTWmgjnP68l,1XuInr77ZWegR201wmDYp1,2zYs4BonN2ydkbrRk333SN,0erJdrkQUJlA7nbtn1qxQR,4FRKGdgOh9G5sub5GiknfS,2hKLu5akQZ9BbrCQVXM2cZ,284b45rER2dZCBVCkCQNEk,0LmD6LnXYecaaMZKG8bW12,4a2rDgkDZVSN660vkhE4mE,7KP43bdMn2KDAchHph8rNP,73J7ROhWCZB9UDHombzlSr,47AMYhY7w49L3rv6UZdlXV,43Hf4nALLvqdVAPTzMtxn4,2vXJl10Kiu3IqMo8moFBFK,31FgIP47GdYyYKLqrs2qk3,002qGVBq254LJPG0t5uNv8,6QcYfnnQN7FfCaPqOjHWVT,1AtBzcUzKLh4BGwXhFA9K6,3l4MGkR30Qku3pNyUqQD06,5dmzKrF5dzyjsLh85JFUZT,3KdRqhzcAjX1atP3HIqlVv,6GkOrVjHU9BIWR7PZj6r2V,77o8apfpn8YmAfYXYtr391,5Uv7iAjsSToLIm0VpAbCY5,6AnUHNm5KuY2TFwj2VeWkN,57DCnDDUscpxL1b0SM8lz8,1MOG45KKhwGqjE0OXm3IRq,4QV7pRkY0E5TFGrA0YNssj,3hTLw115ohhFJWmiusvKGU,09k9cybGimJMqymIfXkWYA:\n None, reason: None"
     ]
    }
   ],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from spotipy.exceptions import SpotifyException\n",
    "import pandas as pd\n",
    "# Function to divide track id list into small groups (chunks)\n",
    "def chunk_track_ids(track_ids,chunk_size=100):\n",
    "    for i in range(0,len(track_ids),chunk_size):\n",
    "        yield track_ids[i:i+chunk_size]\n",
    "\n",
    "\n",
    "def crawl_track_feature(dfTrack: pd.DataFrame, Execution_date: str): \n",
    "\n",
    "    sp =spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id='5036da07445c484eafb112ed83c0f03a'\n",
    "                                                          ,client_secret='3da98e03edf442148da0f481b2cbc007'))\n",
    "    \n",
    "    Track_Feature_Data =[]\n",
    "    i=1\n",
    "\n",
    "    track_ids = dfTrack['Track_ID'].tolist()\n",
    "    # Split track id list into chunks of 100\n",
    "    for chunk in chunk_track_ids(track_ids):\n",
    "        print(chunk)\n",
    "        print(str(i)+f\" )Calling API for {len(chunk)} tracks\")\n",
    "        tracks = sp.audio_features(chunk) # Get information about multiple tracks\n",
    "        if tracks:\n",
    "            for track,track_id in zip(tracks or [],chunk):\n",
    "                if track:\n",
    "                    Track_Feature_Data.append({\n",
    "                        'Track_ID':track_id,\n",
    "                        'Danceability':track.get('danceability',None),\n",
    "                        'Energy':track.get('energy',None),\n",
    "                        'Key':track.get('key',None),\n",
    "                        'Loudness':track.get('loudness',None),\n",
    "                        'Mode':track.get('mode',None),\n",
    "                        'Speechiness':track.get('speechiness',None),\n",
    "                        'Acousticness':track.get('acousticness',None),\n",
    "                        'Instrumentalness':track.get('instrumentalness',None),\n",
    "                        'Liveness':track.get('liveness',None),\n",
    "                        'Valence':track.get('valence',None),\n",
    "                        'Tempo':track.get('tempo',None),\n",
    "                        'Time_signature':track.get('time_signature',None),\n",
    "                        'Track_href':track.get('track_href',None),\n",
    "                        'Type_Feature':track.get('type',None),\n",
    "                        'Analysis_Url':track.get('analysis_url',None)\n",
    "                    })\n",
    "                else:\n",
    "                    Track_Feature_Data.append({\n",
    "                        'Track_ID':track_id,\n",
    "                        'Danceability':None,\n",
    "                        'Energy':None,\n",
    "                        'Key':None,\n",
    "                        'Loudness':None,\n",
    "                        'Mode':None,\n",
    "                        'Speechiness':None,\n",
    "                        'Acousticness':None,\n",
    "                        'Instrumentalness':None,\n",
    "                        'Liveness':None,\n",
    "                        'Valence':None,\n",
    "                        'Tempo':None,\n",
    "                        'Time_signature':None,\n",
    "                        'Track_href':None,\n",
    "                        'Type_Feature':None,\n",
    "                        'Analysis_Url':None\n",
    "                })\n",
    "        i+=1\n",
    "    Track_Feature_Data = pd.DataFrame(Track_Feature_Data)\n",
    "    Track_Feature_Data['Execution_date'] = Execution_date\n",
    "    return Track_Feature_Data\n",
    "\n",
    "def load_daily_track_feature_mongoDB(Execution_date: str):\n",
    "    with mongoDB_client(username = 'huynhthuan', password = 'password') as client:\n",
    "        client_operations = mongoDB_operations(client)\n",
    "        daily_track_data = client_operations.read_data(database_name = 'music_database', collection_name = 'track_collection', query = {'Execution_date': Execution_date})\n",
    "        daily_track_feature_data = crawl_track_feature(daily_track_data, Execution_date)\n",
    "        #client_operations.insert_data(database_name = 'music_database', collection_name = 'trackfeature_collection', data = daily_track_feature_data)\n",
    "\n",
    "load_daily_track_feature_mongoDB(\"2024-11-28\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Spark Session with app name: test and master: local!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|column|\n",
      "+------+\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "|column|\n",
      "+------+\n",
      "|     a|\n",
      "|     b|\n",
      "|     d|\n",
      "+------+\n",
      "\n",
      "+------+\n",
      "|column|\n",
      "+------+\n",
      "|     b|\n",
      "|     a|\n",
      "|     d|\n",
      "+------+\n",
      "\n",
      "Successfully stopped Spark Session!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "with get_sparkSession(\"test\") as spark:\n",
    "    d2 = [('a',), ('b',), ('d',)]\n",
    "    col = ['column']\n",
    "    schema = StructType([\n",
    "    StructField(\"column\", StringType(), True)\n",
    "    ])\n",
    "    df1 = spark.createDataFrame([], schema)\n",
    "    df2 = spark.createDataFrame(d2, col)\n",
    "    df1.show()\n",
    "    df2.show()\n",
    "\n",
    "    df2.join(df1, on = 'column', how = 'left_anti').show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
